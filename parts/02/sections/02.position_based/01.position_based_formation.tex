Here we are interested in steering each agent $i \in \mathcal{V}$ into
resting at a \textit{position} in 3D space, while conforming to the requirements
of the problem; that is, all agents should avoid colliding with each other, all
obstacles in the workspace, and the workspace boundary itself, while remaining
in a non-singular configuration and sustaining the connectivity to their
respective neighbours.


%-------------------------------------------------------------------------------
\subsection{The error model}

A desired configuration $\vect{z}_{i,des} \in \mathbb{R}^9 \times \mathbb{T}^3$
is associated to each agent $i \in \mathcal{V}$, with the aim of agent $i$
achieving it in steady-state:
$\lim\limits_{t \to \infty} \|\vect{z}_i(t) - \vect{z}_{i,des}\| = 0$. The
interior of the norm of this expression denotes the state error of agent $i$:

$$\vect{e}_i(t) = \vect{z}_i(t) - \vect{z}_{i,des}, \vect{e}_i(t) :
\mathbb{R}_{\geq 0} \to \mathbb{R}^9 \times \mathbb{T}^3$$

The error dynamics are denoted by $g_i(\vect{e}_i, \vect{u}_i)$:
\begin{align}
  \dot{\vect{e}}_i(t) = \dot{\vect{z}}_i(t) - \dot{\vect{z}}_{i,des} =
  \dot{\vect{z}}_i(t) = f_i\big(\vect{z}_i(t), \vect{u}_i(t)\big) = g_i(\vect{e}_i(t), \vect{u}_i(t)\big)
  \label{eq:position_based_error_model}
\end{align}
with $\vect{e}_i(0) = \vect{z}_i(0) - \vect{z}_{i,des}$


%-------------------------------------------------------------------------------
\subsection{The optimization problem}

At a generic time $t_0$, agent $i$ solves the following optimization problem:

\begin{align}
  \min\limits_{\overline{\vect{u}}_i (\cdot)}\ &
    J_i \big(\overline{\vect{u}}_i (\cdot);\ \vect{e}_i(t_0)\big) \triangleq
      \int_{t_0}^{t_0 + T_p} F_i \big(\overline{\vect{e}}_i(\tau), \overline{\vect{u}}_i (\tau)\big) d \tau +
      V_i \big(\overline{\vect{e}}_i (t_0 + T_p)\big) \label{position_based_cost} \\
  \text{subject to:} & \nonumber \\
  & \dot{\overline{\vect{e}}}_i(t) = g_i \big(\overline{\vect{e}}_i (t), \overline{\vect{u}}_i (t)\big) \\
  & \overline{\vect{e}}_i (t_0) = \vect{e}_i (t_0) \\
  & \overline{\vect{u}}_i(t) \in \mathcal{U}_i, t \in [t_0, t_0 + T_p)\\
  & \overline{\vect{e}}_i (t) \in \mathcal{E}_i,\ t \in [t_0, t_0 + T_p]\\
  & \overline{\vect{e}}_i (t_0 + T_p) \in \mathcal{E}_i^f \\
  \text{and } \forall t \in [t_0, t_0 + T_p]:& \nonumber \\
  & \|\overline{\vect{p}}_i(t) - \overline{\vect{p}}_j(t)\| > \underline{d}_{ij,a}, \forall j \in \mathcal{R}_i(t) \label{constraint:p_1}\\
  & \|\overline{\vect{p}}_i(t) - \vect{p}_{\ell}(t)\| > \underline{d}_{i\ell,o}, \forall \ell \in \mathcal{L} \\
  & \|\overline{\vect{p}}_i(t)\| + r_i < r_W \\
  & \|\overline{\vect{p}}_i(t) - \overline{\vect{p}}_j(t)\| < d_i, \forall j \in \mathcal{N}_i \\
  & \overline{\theta}_i(t) \ne \pm \frac{\pi}{2} \label{constraint:p_5}
\end{align}\\
Constraints \ref{constraint:p_1}-\ref{constraint:p_5} explicitly address the
requirements posed by the problem \eqref{problem}, while the rest exist to
ensure that formation is achieved under constrained states and input signals.

The functions
$F_i : \mathcal{E}_i \times \mathcal{U}_i \to \mathbb{R}_{\geq 0}$ and
$V_i: \mathcal{E}_i^f \to \mathbb{R}_{\geq 0}$ are defined as
\begin{align}
  F_i (\vect{e}_i, \vect{u}_i)
    &\triangleq \| \vect{e}_i\|_{\mat{Q}_i}^2 + \| \vect{u}_i\|_{\mat{R}_i}^2 \\
  V_i (\vect{e}_i)
    & \triangleq \| \vect{e}_i \|_{\mat{P}_i}^2
\end{align}\\
Matrices $\mat{R}_i \in \mathbb{R}^{6 \times 6}$ are symmetric and positive
definite, while matrices $\mat{Q}_i, \mat{P}_i \in \mathbb{R}^{12 \times 12}$
are symmetric and positive semi-definite.

The set $\mathcal{E}_i$ is such that
$$\mathcal{E}_i = \{\vect{e}_i \in \mathbb{R}^{12} : \vect{e}_i \in \mathcal{Z}_i \oplus (-z_{i,des} )\}$$

The terminal set $\mathcal{E}_i^f \subseteq \mathcal{E}_i$ is an admissible
positively invariant set \note{?? define it} for system
\eqref{eq:position_based_error_model} such that
\begin{align}
  \mathcal{E}_i^f = \{\vect{e}_i \in \mathcal{E}_i : \|\vect{e}_i\| \leq \epsilon_0 \}
\end{align}
where $\epsilon_0$ is an arbitrarily small but fixed positive real scalar.\\

With regard to the terminal penalty function $V_i$, the following lemma will
prove to be useful in guaranteeing the convergence of the solution to the
optimal control problem to the terminal region $\mathcal{E}_i^f$:

\begin{bw_box}
\begin{lemma} ($V_i$ is Lipschitz continuous in $\mathcal{E}_i^f$)

  The terminal penalty function $V_i$ is Lipschitz continuous in
  $\mathcal{E}_i^f$
  $$|V(\vect{e}_{1,i}) - V(\vect{e}_{2,i})| \leq L_{V_i} \|\vect{e}_{1,i} - \vect{e}_{2,i}\|$$
  where $\vect{e}_{1,i}, \vect{e}_{2,i} \in \mathcal{E}_i^f$,
  with Lipschitz constant $L_{V_i} = 2 \epsilon_0 \lambda_{max}(P_i)$\\

  \begin{gg_box}
  \textbf{Proof} For every $\vect{e}_i \in \mathcal{E}_i^f$, it holds that
  \begin{align}
    |V(\vect{e}_{1,i}) - V(\vect{e}_{2,i})| &= |\vect{e}_{1,i}^{\top} \mat{P}_i \vect{e}_{1,i} - \vect{e}_{2,i}^{\top} \mat{P}_i \vect{e}_{2,i}| \\
      &= |\vect{e}_{1,i}^{\top} \mat{P}_i \vect{e}_{1,i} - \vect{e}_{2,i}^{\top} \mat{P}_i \vect{e}_{2,i} \pm \vect{e}_{1,i}^{\top} \mat{P}_i \vect{e}_{2,i}| \\
      &= |\vect{e}_{1,i}^{\top} \mat{P}_i (\vect{e}_{1,i}-\vect{e}_{2,i}) - \vect{e}_{2,i}^{\top} \mat{P}_i (\vect{e}_{1,i}-\vect{e}_{2,i})| \\
      &\leq |\vect{e}_{1,i}^{\top} \mat{P}_i (\vect{e}_{1,i}-\vect{e}_{2,i})| + |\vect{e}_{2,i}^{\top} \mat{P}_i (\vect{e}_{1,i}-\vect{e}_{2,i})|
  \end{align}

  But for any $\vect{x}, \vect{y} \in \mathbb{R}^n$
  $$|\vect{x}^{\top} \mat{A} \vect{y}| \leq \lambda_{max}(A) \|\vect{x}\| \|\vect{y}\|$$
  where $\lambda_{max}(A)$ denotes the largest eigenvalue of matrix $\mat{A}$.
  Hence:
  \begin{align}
    |V(\vect{e}_{1,i}) - V(\vect{e}_{2,i})| &\leq
    \lambda_{max}(\mat{P}_i) \|\vect{e}_{1,i}\| \|\vect{e}_{1,i} - \vect{e}_{2,i}\| +
    \lambda_{max}(\mat{P}_i) \|\vect{e}_{2,i}\| \|\vect{e}_{1,i} - \vect{e}_{2,i}\| \\
    &= \lambda_{max}(\mat{P}_i) (\|\vect{e}_{1,i}\| + \|\vect{e}_{2,i}\|)\|\vect{e}_{1,i} - \vect{e}_{2,i}\| \\
    & \leq \lambda_{max}(\mat{P}_i) (\epsilon_0 + \epsilon_0)\|\vect{e}_{1,i} - \vect{e}_{2,i}\| \\
    &= 2 \epsilon_0 \lambda_{max}(\mat{P}_i) \|\vect{e}_{1,i} - \vect{e}_{2,i}\|
  \end{align}
  \qedsymbol
  \end{gg_box}
\label{lemma:V_Lipschitz_e_0}
\end{lemma}
\end{bw_box}


The solution to the optimal control problem \eqref{position_based_cost} -
\eqref{constraint:p_5} at time $t_0$ is an optimal control input
$\vect{u}_i^{\star}(\cdot;\ \vect{e}_i(t_0))$ which
is applied to the open-loop system until the next sampling instant $t_0 + h$,
at which time a new optimal control problem is solved in the same manner:
\begin{align}
  \vect{u}_i\big(t;\ \vect{e}_i(t_0)\big) = \vect{u}_i^{\star}\big(t;\ \vect{e}_i(t_0)\big) \label{eq:position_based_optimal_u} \\
  t \in [t_0, t_0 + h) \nonumber \\
  0 < h < T_p \nonumber
\end{align}

The control input $\vect{u}_i(\cdot)$ is a feedback, since it is
recalculated at each sampling instant based on the then-current state. The
solution to equation \eqref{eq:position_based_error_model}, starting at time
$t_0$, from an initial condition $\vect{e}_i(t_0)$, by application of the
control input $\vect{u}_i : [t_0, t_1] \to \mathcal{U}_i$ is denoted by
$$\vect{e}_i\big(t;\ \vect{u}_i(\cdot), \vect{e}_i(t_0)\big)$$
with $t \in [t_0, t_1]$.

The \textit{predicted} state of the system \eqref{eq:position_based_error_model}
at time $t_0 + \tau$, based on the measurement of the state at time
$t_0$, $\vect{e}_i(t_0)$, by application of the control input
$\vect{u}_i\big(t;\ \vect{e}_i(t_0)\big)$, for the time period $t \in [t_0, t_1]$
is denoted by

\begin{align}
  \overline{\vect{e}}_i\big(t_0 + \tau;\ \vect{u}_i(\cdot), \vect{e}_i(t_0)\big) \label{eq:position_based_predicted_error_0}
\end{align}
As is natural,
$\vect{e}_i(t_0) = \overline{\vect{e}}_i\big(t_0;\ \vect{u}_i(\cdot), \vect{e}_i(t_0)\big)$.\\

We can now give the definition of an \textit{admissible input}:

\begin{bw_box}
\begin{definition} (Admissible input)\\

  A control input $\vect{u}_i : [t_0, t_0 + T_p] \to \mathbb{R}^6$ for a state
  $\vect{e}_i(t_0)$ is called \textit{admissible} if all the following hold:

  \begin{enumerate}
    \item $\vect{u}_i(\cdot)$ is piecewise continuous
    \item $\vect{u}_i(\tau) \in \mathcal{U}_i,\ \forall \tau \in [t_0, t_0 + T_p]$
    \item $\vect{e}_i\big(\tau;\ \vect{u}_i(\cdot), \vect{e}_i(t_0)\big) \in \mathcal{E}_i,\ \forall \tau \in [t_0, t_0 + T_p]$
    \item $\vect{e}_i\big(t_0 + T_p;\ \vect{u}_i(\cdot), \vect{e}_i(t_0)\big) \in \mathcal{E}_i^f$
  \end{enumerate}

\end{definition}
\end{bw_box}


%-------------------------------------------------------------------------------
\subsection{Feasibility and Convergence}

Under these considerations, we can now state the theorem that relates to
the guaranteeing of the stability of the compound system of agents
$i \in \mathcal{V}$, when each of them is assigned a desired
position which results in feasible displacements:\\

\begin{bw_box}
\begin{theorem}

  Suppose that

  \begin{enumerate}
    \item the terminal region $\mathcal{E}_i^f \subseteq \mathcal{E}_i$ is
      closed with $0 \in \mathcal{E}_i^f$
    \item a solution to the optimal control problem \eqref{position_based_cost} -
      \eqref{constraint:p_5} is feasible at time $t=0$, that is, assumptions
      \eqref{ass:measurements_access}, \eqref{ass:initial_conditions}, and
      \eqref{ass:after_formation_geometry} hold at time $t=0$
    \item there exists an admissible control input
      $\vect{u}_i^f : [0, h] \to \mathcal{U}_i$ such that for all
      $\vect{e}_i \in \mathcal{E}_i^f$ and $\tau \in [0,h]$:

      \begin{enumerate}
        \item $\vect{e}_i(\tau) \in \mathcal{E}_i^f$
        \item $\dfrac{\partial V_i}{\partial \vect{e}_i} g_i\big(\vect{e}_i(\tau), \vect{u}_i^f(\tau)\big)
          + F_i\big(\vect{e}_i(\tau), \vect{u}_i^f(\tau)\big) \leq 0$
      \end{enumerate}

  \end{enumerate}

  then the closed loop system \eqref{eq:position_based_error_model} under the
  control input \eqref{eq:position_based_optimal_u} converges to the set
  $\mathcal{E}_i^f$ when $t \to \infty$.

\end{theorem}
\end{bw_box}

\textbf{Proof}. The proof of the above theorem consists of two parts:
in the first, recursive feasibility is established, that is, initial
feasibility is shown to imply subsequent feasibility; in the second, and based
on the first part, it is shown that the error $\vect{e}_i(t)$ converges to the
terminal set $\mathcal{E}_i^f$.\\

\textbf{Feasibility analysis}
Consider a sampling instant $t_0$ for which a
solution $\vect{u}_i^{\star}\big(\cdot;\ \vect{e}_i(t_0)\big)$ to
\eqref{position_based_cost} exists.
%In between $t_0$ and $t_0 + h$,
%where $h$ is the sampling time, the optimal control signal
%$\vect{u}_i^{\star}\big(\cdot;\ \vect{e}_i(t_0)\big)$ is applied to the open-loop
%system.
Suppose now a time instant $t_1$ such that\footnote{It is not strictly necessary
that $t_1 = t_0 + h$ here, however it is necessary for the following that
$t_1 - t_0 \leq h$} $t_0 < t_1 < t_0 + T_p$, and consider that the
optimal control signal calculated at $t_0$ contains the following two portions:

\begin{equation}
  \vect{u}_i^{\star}\big(\cdot;\ \vect{e}_i(t_0)\big) = \left\{ \\
      \begin{array}{ll}
        \vect{u}_i^{\star}\big(\tau_1;\ \vect{e}_i(t_0)\big), & \tau_1 \in [t_0, t_1] \\
        \vect{u}_i^{\star}\big(\tau_2;\ \vect{e}_i(t_0)\big), & \tau_2 \in [t_1, t_0 + T_p]
      \end{array}
      \right.
  \label{eq:optimal_input_portions}
\end{equation}

Both portions are admissible since the calculated optimal control input is
admissible, and hence they both conform to the input constraints.
As for the resulting predicted states, they satisfy the state constraints, and,
crucially: $\overline{\vect{e}}_i\big(t_0 + T_p;\ \vect{u}_i^{\star}(\cdot), \vect{e}_i(t_0)\big) \in \mathcal{E}_i^f$.
Furthermore, according to assumption (3) of the theorem, there exists an
admissible (and certainly not guaranteed optimal) input $\vect{u}_i^f$ that
renders $\mathcal{E}_i^f$ invariant over $[t_0 + T_p, t_0 + T_p + h]$.

Given the above facts, we can construct an admissible input for time $t_1$
by sewing together the second portion of \eqref{eq:optimal_input_portions}
and the input $\vect{u}_i^f(\cdot)$:

\begin{equation}
  \tilde{\vect{u}}_i\big(\tau;\ \vect{e}_i(t_1)\big) = \left\{ \\
      \begin{array}{ll}
        \vect{u}_i^{\star}\big(\tau;\ \vect{e}_i(t_0)\big), & \tau \in [t_1, t_0 + T_p] \\
        %\vect{u}_i^f\Big(\overline{\vect{e}}_i\big(t_0 + T_p;\ \vect{u}_i^{\star}, \vect{e}_i(t_0)\big)\Big), & \tau_2 \in (t_0 + T_p, t_1 + T_p]
        \vect{u}_i^f(\tau - t_0 - T_p), & \tau \in (t_0 + T_p, t_1 + T_p]
      \end{array}
      \right.
\label{eq:optimal_input_t_plus_one}
\end{equation}

Applied at time $t_1$, $\tilde{\vect{u}}_i\big(\cdot;\ \vect{e}_i(t_1)\big)$
is an admissible control input as a composition of admissible control inputs.

This means that feasibility of a solution to the optimization problem at time
$t_0$ implies feasibility at time $t_1 > t_0$, and, thus, since at time $t=0$
a solution is assumed to be feasible, a solution to the optimal control problem
is feasible for all $t \geq 0$.\\

\textbf{Convergence analysis}
The second part of the proof involves demonstrating the convergence of the
state $\vect{e}_i$ to the terminal set $\mathcal{E}_i^f$. In order for this
to be proved, it must be shown that a proper value function decreases along
the solution trajectories starting at some initial time $t_0$. We consider the
\textit{optimal} cost $J_i^{\star}\big(\vect{e}_i(t)\big)$ as a candidate
Lyapunov function:
$$J_i^{\star}\big(\vect{e}_i(t)\big) \triangleq J_i \Big(\vect{e}_i(t), \vect{u}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big)\Big)$$
and, in particular, our goal is to show that that this cost decreases over
consecutive sampling instants $t_1 = t_0 + h$, i.e.
$J_i^{\star}\big(\vect{e}_i(t_1)\big) - J_i^{\star}\big(\vect{e}_i(t_0)\big) \leq 0$.\\

In order not to wreak notational havoc, let us define the following terms:
\begin{gg_box}
\begin{itemize}
  \item $\vect{u}_{0,i}(\tau) \triangleq \vect{u}_i^{\star}\big(\tau;\ \vect{e}_i(t_0)\big)$
    as the \textit{optimal} input based on the measurement of state
    $\vect{e}_i(t_0)$, applied at time $\tau \geq t_0$
  \item $\vect{e}_{0,i}(\tau) \triangleq \overline{\vect{e}}_i\big(\tau;\ \vect{u}_{0,i}(\tau), \vect{e}_i(t_0)\big)$
    as the \textit{predicted} state at time $\tau \geq t_0$, that is,
    the state that results from the application of the above input
    $\vect{u}_i^{\star}\big(\tau;\ \vect{e}_i(t_0)\big)$ at time $\tau$
  \item $\vect{u}_{1,i}(\tau) \triangleq \tilde{\vect{u}}_i\big(\tau;\ \vect{e}_i(t_1)\big)$
    as the \textit{feasible} input applied at $\tau \geq t_1$ (see eq. \eqref{eq:optimal_input_t_plus_one} above)
  \item $\vect{e}_{1,i}(\tau) \triangleq \overline{\vect{e}}_i\big(\tau;\ \vect{u}_{1,i}(\tau), \vect{e}_i(t_1)\big)$
    as the \textit{predicted} state at time $\tau \geq t_1$, that is,
    the state that results from the application of the above input
    $\tilde{\vect{u}}_i\big(\tau;\ \vect{e}_i(t_1)\big)$ at time $\tau$
\end{itemize}
\end{gg_box}

Before beginning to prove convergence, it is worth noting that while the cost
$$J_i \Big(\vect{e}_i(t), \vect{u}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big)\Big)$$
is optimal (in the sense that it is based on the optimal input, which provides
its minimum realization), a cost that is based on a plainly feasible
(and thus, without loss of generality, sub-optimal) input
$\vect{u}_i \not= \vect{u}_i^{\star}$ will result in a configuration where
\begin{equation}
J_i \Big(\vect{e}_i(t), \vect{u}_i\big(\cdot;\ \vect{e}_i(t)\big)\Big)
\geq J_i \Big(\vect{e}_i(t), \vect{u}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big)\Big)
\end{equation}

Let us now begin our investigation on the sign of the difference between the cost
that results from the application of the feasible input $\vect{u}_{1,i}$,
which we shall denote by $\overline{J}_i\big(\vect{e}_i(t_1)\big)$,
and the optimal cost $J_i^{\star}\big(\vect{e}_i(t_0)\big)$, while reminding
ourselves that
$J_i \big(\overline{\vect{u}}_i (\cdot);\ \vect{e}_i(t)\big)$ $=$
$\int_{t}^{t + T_p} F_i \big(\overline{\vect{e}}_i(\tau), \overline{\vect{u}}_i (\tau)\big) d \tau$ $+$
$V_i \big(\overline{\vect{e}}_i (t + T_p)\big)$:

\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_1)\big) - J_i^{\star}\big(\vect{e}_i(t_0)\big) &= \\
   & V_i \big(\vect{e}_{1,i} (t_1 + T_p)\big) + \int_{t_1}^{t_1 + T_p} F_i \big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i} (\tau)\big) d \tau \\
  -&V_i \big(\vect{e}_{0,i} (t_0 + T_p)\big) - \int_{t_0}^{t_0 + T_p} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) d \tau
\end{align}

Considering that $t_0 < t_1 < t_0 + T_p < t_1 + T_p$, we break down the
two integrals above in between these intervals:

\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_1)\big) - J_i^{\star}\big(\vect{e}_i(t_0)\big) &= \\
    V_i \big(\vect{e}_{1,i} (t_1 + T_p)\big)
    &+ \int_{t_1}^{t_0 + T_p} F_i \big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i} (\tau)\big) d \tau
    + \int_{t_0 + T_p}^{t_1 + T_p} F_i \big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i} (\tau)\big) d \tau \\
    -V_i \big(\vect{e}_{0,i} (t_0 + T_p)\big)
    &- \int_{t_0}^{t_1} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) d \tau
    - \int_{t_1}^{t_0 + T_p} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) d \tau
\label{eq:convergence_4_integrals}
\end{align}

\begin{gg_box}
In between the times $t_1$ and $t_0 + T_p$, the constructed feasible input
$\tilde{\vect{u}}_i\big(\cdot;\ \vect{e}_i(t_1)\big)$ is equal to the optimal
input $\vect{u}_i^{\star}\big(\cdot;\ \vect{e}_i(t_0)\big)$
(see eq. \ref{eq:optimal_input_t_plus_one}), which means that
$\vect{u}_{1,i}(\cdot) = \vect{u}_{0,i}(\cdot)$ in the interval $[t_1, t_0 + T_p]$.
Furthermore, this means that the predicted states according to these inputs will
be also equal in this interval: $\vect{e}_{1,i}(\cdot) = \vect{e}_{0,i}(\cdot)$.
Hence, the following equality holds over $[t_1, t_0 + T_p]$:

\begin{align}
  F_i \big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i} (\tau)\big) =
  F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big),\ \tau \in [t_1, t_0 + T_p]
\end{align}

Integrating this equality over the interval where it is valid yields

\begin{align}
  \int_{t_1}^{t_0 + T_p}F_i \big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i} (\tau)\big) d\tau =
  \int_{t_1}^{t_0 + T_p}F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) d\tau
\end{align}

\end{gg_box}

This means that these two integrals featured in the right-hand side of eq.
\eqref{eq:convergence_4_integrals} vanish, and thus the cost difference becomes

\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_1)\big) - J_i^{\star}\big(\vect{e}_i(t_0)\big) &= \\
    V_i \big(\vect{e}_{1,i} (t_1 + T_p)\big)
    &+\int_{t_0 + T_p}^{t_1 + T_p} F_i \big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i} (\tau)\big) d \tau \\
    -V_i \big(\vect{e}_{0,i} (t_0 + T_p)\big)
    &-\int_{t_0}^{t_1} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) d \tau
\label{eq:convergence_2_integrals}
\end{align}

\begin{gg_box}
  We turn our attention to the first integral in the above expression, and we
  note that $(t_1 + T_p) - (t_0 + T_p) = t_1 - t_0 = h$, which is exactly the
  length of the interval where assumption (3b) of the theorem holds. Hence,
  we decide to integrate the expression found in the assumption over the
  interval $[t_1 + T_p, t_0 + T_p]$, for the controls and states that are
  applicable in it:
  \begin{align}
    \int_{t_0 + T_p}^{t_1 + T_p} \Bigg(\dfrac{\partial V_i}{\partial \vect{e}_{1,i}} g_i\big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i}(\tau)\big)
    + F_i\big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i}(\tau)\big)\Bigg) d\tau \leq 0 \\
    \int_{t_0 + T_p}^{t_1 + T_p} \dfrac{d}{d\tau} V_i\big(\vect{e}_{1,i}(\tau)\big) d \tau
    + \int_{t_0 + T_p}^{t_1 + T_p} F_i\big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i}(\tau)\big) d\tau \leq 0 \\
    V_i\big(\vect{e}_{1,i}(t_1 + T_p)\big) - V_i\big(\vect{e}_{1,i}(t_0 + T_p)\big)
    + \int_{t_0 + T_p}^{t_1 + T_p} F_i\big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i}(\tau)\big) d\tau \leq 0 \\
    V_i\big(\vect{e}_{1,i}(t_1 + T_p)\big)
    + \int_{t_0 + T_p}^{t_1 + T_p} F_i\big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i}(\tau)\big) d\tau \leq V_i\big(\vect{e}_{1,i}(t_0 + T_p)\big)
  \end{align}

  The left-hand side expression is the same as the first two terms in the
  right-hand side of equality \eqref{eq:convergence_2_integrals}. We can
  introduce the third one by subtracting it from both sides of the above
  expression:
  \begin{align}
    V_i\big(\vect{e}_{1,i}(t_1 + T_p)\big)
    + \int_{t_0 + T_p}^{t_1 + T_p} F_i\big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i}(\tau)\big) d\tau
    - V_i\big(\vect{e}_{0,i}(t_0 + T_p)\big) \\
    \leq V_i\big(\vect{e}_{1,i}(t_0 + T_p)\big)
    - V_i\big(\vect{e}_{0,i}(t_0 + T_p)\big) \\
    \leq \Big|V_i\big(\vect{e}_{1,i}(t_0 + T_p)\big)
    - V_i\big(\vect{e}_{0,i}(t_0 + T_p)\big)\Big|
  \end{align}
  since $x \leq |x|, \forall x \in \mathbb{R}$.

  By revisiting lemma \eqref{lemma:V_Lipschitz_e_0}, the above inequality
  becomes
  \begin{align}
    V_i\big(\vect{e}_{1,i}(t_1 + T_p)\big)
    + \int_{t_0 + T_p}^{t_1 + T_p} F_i\big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i}(\tau)\big) d\tau
    - V_i\big(\vect{e}_{0,i}(t_0 + T_p)\big) \\
    \leq L_{V_i} \big\|\vect{e}_{1,i}(t_0 + T_p) - \vect{e}_{0,i}(t_0 + T_p)\big\|
  \end{align}

  But in the interval $[t_1, t_0 + T_p]$:
  $\vect{e}_{1,i}(\cdot) = \vect{e}_{0,i}(\cdot)$, hence the right-hand side
  of the inequality equals zero
  \begin{align}
    V_i\big(\vect{e}_{1,i}(t_1 + T_p)\big)
    + \int_{t_0 + T_p}^{t_1 + T_p} F_i\big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i}(\tau)\big) d\tau
    - V_i\big(\vect{e}_{0,i}(t_0 + T_p)\big) \leq 0
  \end{align}

  By subtracting the term
  $-\int_{t_0}^{t_1} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) d \tau$
  from both sides we get
  \begin{align}
    &V_i\big(\vect{e}_{1,i}(t_1 + T_p)\big)
    + \int_{t_0 + T_p}^{t_1 + T_p} F_i\big(\vect{e}_{1,i}(\tau), \vect{u}_{1,i}(\tau)\big) d\tau \\
    - &V_i\big(\vect{e}_{0,i}(t_0 + T_p)\big)
    -\int_{t_0}^{t_1} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) d \tau
    \leq -\int_{t_0}^{t_1} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) d \tau
  \end{align}

  The left-hand side of the inequality is equal to the cost difference.
\end{gg_box}

Hence, the cost difference becomes
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_1)\big) - J_i^{\star}\big(\vect{e}_i(t_0)\big) \leq
    -\int_{t_0}^{t_1} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) d \tau
\end{align}

\begin{gg_box}
  $F_i$ is a positive-definite function as a sum of a positive-definite
  $\|\vect{u}_i\|^2_{\mat{R}_i}$ and a positive semi-definite function
  $\|\vect{e}_i\|^2_{\mat{Q}_i}$. If we denote by $m \geq 0$ the
  minimum eigenvalue between those of matrices $\mat{R}_i, \mat{Q}_i$, this
  means that
  $$F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) \geq m \|\vect{e}_{0,i}(\tau)\|^2$$

  By integrating the above between our interval of interest $[t_0, t_1]$ we get
  \begin{align}
    \int_{t_0}^{t_1} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) &\geq \int_{t_0}^{t_1} m \|\vect{e}_{0,i}(\tau)\|^2 d\tau \\
    -\int_{t_0}^{t_1} F_i \big(\vect{e}_{0,i}(\tau), \vect{u}_{0,i} (\tau)\big) &\leq -m \int_{t_0}^{t_1} \|\vect{e}_{0,i}(\tau)\|^2 d\tau
  \end{align}
\end{gg_box}

Which means that the cost difference
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_1)\big) - J_i^{\star}\big(\vect{e}_i(t_0)\big)
  &\leq -m \int_{t_0}^{t_1} \|\vect{e}_{0,i}(\tau)\|^2 d\tau \\
  \overline{J}_i\big(\vect{e}_i(t_1)\big) - J_i^{\star}\big(\vect{e}_i(t_0)\big) &\leq 0
\end{align}

And since the cost $\overline{J}_i\big(\vect{e}_i(t_1)\big)$ is, in general,
sub-optimal: $J_i^{\star}\big(\vect{e}_i(t_1)\big) \leq \overline{J}_i\big(\vect{e}_i(t_1)\big)$,
the Lyapunov function $J_i^{\star}(\cdot)$ is decreasing along consecutive
sampling times:
\begin{align}
 J_i^{\star}\big(\vect{e}_i(t_1)\big) \leq J_i^{\star}\big(\vect{e}_i(t_0)\big)
\end{align}

Therefore, the closed-loop trajectory of $\vect{e}_i$ converges to the terminal
set $\mathcal{E}_i^f$ as $t \to \infty$. \qedsymbol
