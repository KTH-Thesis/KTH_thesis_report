%-------------------------------------------------------------------------------
\subsection{Stabilization: Feasibility and Convergence}

Under these considerations, we can now state the theorem that relates to
the guaranteeing of the stability of the compound system of agents
$i \in \mathcal{V}$, when each of them is assigned a desired
position which results in feasible displacements:\\[2.5ex]

\begin{bw_box}
\begin{theorem}
  \label{theorem:with_disturbances}
  Suppose that

  \begin{enumerate}
    \item a solution to the optimal control problem \eqref{position_based_cost_2}
      is feasible at time $t=0$, that is, assumptions
      \eqref{ass:measurements_access}, \eqref{ass:initial_conditions}, and
      \eqref{ass:intra_environmental_arrangement} hold at time $t=0$
    \item there exists an admissible control input of linear feedback form
      $\vect{h}_i(\vect{e}_i) : [t_k + T_p, t_{k+1} + T_p] \to \mathcal{U}_i$
      such that for all $\vect{e}_i \in \Psi_i$ and $\forall \tau \in
      [t_k + T_p, t_{k+1} + T_p]$:
      \begin{align}
        \dfrac{\partial V_i}{\partial \vect{e}_i} g_i\big(\vect{e}_i(\tau), \vect{h}_i(\vect{e}_i)\big)
          + F_i\big(\vect{e}_i(\tau), \vect{h}_i(\vect{e}_i)\big) \leq 0
      \end{align}
    \item the set $\Psi_i$ is such that
      \begin{align}
        \text{for all } \vect{e}_i \in \Psi_i \Rightarrow g_i(\vect{e}_i, \vect{h}_i(\vect{e}_i)) \in \Omega_i \subseteq \Psi_i
      \end{align}
    \item the terminal region $\Omega_i \subseteq \mathcal{E}_i$ is
      closed with $\vect{0} \in \Omega_i$
    \item the upper bound $\overline{\delta}_i$ of the disturbance is in turn bounded by
      \begin{align}
        \overline{\delta}_i \leq \dfrac{\varepsilon_{\Psi_i} - \varepsilon_{\Omega_i}}{\dfrac{L_{V_i}}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)}}
      \end{align}

  \end{enumerate}

  then the closed loop system \eqref{eq:with_disturbances_closed_loop} under
  the control input \eqref{eq:position_based_optimal_u_2} converges to the set
  $\Omega_i$ when $t \to \infty$.
\end{theorem}
\end{bw_box}

\textbf{Proof}. The proof of the above theorem consists of two parts:
in the first, recursive feasibility is established, that is, initial
feasibility is shown to imply subsequent feasibility; in the second, and based
on the first part, it is shown that the error state $\vect{e}_i(t)$ converges
to the terminal set $\Omega_i$.\\[2.5ex]

The following proof relies heavily on the Gr\"{o}nwall-Bellman inequality.
We state it here for reference purposes.
\begin{bw_box}
  \begin{lemma} \cite{khalil_nonlinear_systems} \textit{Gr\"{o}nwall-Bellman Inequality}

    Let $\lambda : [a,b] \to \mathbb{R}$ be continuous and
    $\mu : [a,b] \to \mathbb{R}$ be continuous and non-negative. If a
    continuous function $y : [a,b] \to \mathbb{R}$ satisfies
    \begin{align}
      y(t) \leq \lambda(t) + \int_a^t \mu(s) y(s) ds
    \end{align}
    for $a \leq t \leq b$, then on the same interval
    \begin{align}
      y(t) \leq \lambda(t) + \int_a^t \lambda(s) \mu(s) e^{\int_s^t \mu(\tau)d\tau} ds
    \end{align}
    In particular, if $\lambda(t) \equiv \lambda$ is a constant, then
    \begin{align}
      y(t) \leq \lambda e^{\int_a^t \mu(\tau)d\tau} ds
    \end{align}
    If $\lambda(t) \equiv \lambda$ and $\mu(t) \equiv \mu$ are both constants,
    then
    \begin{align}
      y(t) \leq \lambda e^{\mu (t - a)}
    \end{align}
    \label{lemma:bellman_inequality}
  \end{lemma}
\end{bw_box}

\begin{bw_box}
  \begin{remark}
    Given that disturbances \textit{are} present, for the predicted and actual
    states at time $\tau_1 \geq \tau_0 \in \mathbb{R}_{\geq 0}$ it holds that:
    \begin{align}
      \vect{e}_i\big(\tau_1;\ \vect{u}_i(\cdot), \vect{e}_i(\tau_0)\big) &=
        \vect{e}_i(\tau_0) + \int_{\tau_0}^{\tau_1} g_i^R\big(\vect{e}_i(s;\ \vect{e}_i(\tau_0)), \vect{u}_i(s)\big) ds \\[2.5ex]
      \overline{\vect{e}}_i\big(\tau_1;\ \vect{u}_i(\cdot), \vect{e}_i(\tau_0)\big) &=
        \vect{e}_i(\tau_0) + \int_{\tau_0}^{\tau_1} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(\tau_0)), \vect{u}_i(s)\big) ds
    \end{align}
    \label{remark:predicted_actual_equations_with_disturbance}
  \end{remark}
\end{bw_box}


% --- START diff real_vs_predicted at t_(k+1) from t_k -------------------------
\begin{bw_box}
  \begin{lemma}
    Suppose that the real system, which is under the existence of bounded
    additive disturbances, and the model are both at time $t$ at state
    $\vect{e}_i(t)$. Applying at time $t$ a control law $\vect{u}(\cdot)$
    to the system model deemed ``real" and its model will cause at time $t + \tau$,
    $\tau \geq 0$ a divergence between the states of the real system and its
    model. The norm of the difference between the state of the real system
    and the state of the model system is bounded by
    \begin{align}
      \bigg\| \vect{e}_i\big(t + \tau;\ \vect{u}(\cdot), \vect{e}_i(t)\big) -
        \overline{\vect{e}}_i\big(t + \tau;\ \vect{u}(\cdot), \vect{e}_i(t)\big) \bigg\|
        \leq \dfrac{\overline{\delta}_i}{\L_{g_i}} (e^{L_{g_i} \tau} - 1)
    \end{align}
    where $\overline{\delta}_i$ is the upper bound of the disturbance,
    and $L_{g_i}$ the Lipschitz constant of both models.
    \label{lemma:diff_state_from_same_conditions}
  \end{lemma}
\end{bw_box}

\begin{gg_box}
\textbf{Proof of lemma \eqref{lemma:diff_state_from_same_conditions}}

Since there are disturbances present, consulting remark
\eqref{remark:predicted_actual_equations_with_disturbance} and substituting
for $\tau_0 = t$ and $\tau_1 = t + \tau$ yields:
\begin{align}
  \vect{e}_i\big(t + \tau;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) &=
    \vect{e}_i(t)
    + \int_{t}^{t + \tau} g_i\big(\vect{e}_i(s;\ \vect{e}_i(t)), \overline{\vect{u}}_i^{\star}(s)\big) ds
    + \int_{t}^{t + \tau}\delta_i(s)ds\\[2.5ex]
  \overline{\vect{e}}_i\big(t + \tau;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) &=
    \vect{e}_i(t) + \int_{t}^{t + \tau} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t)), \overline{\vect{u}}_i^{\star}(s)\big) ds
\end{align}
Subtracting the latter from the former and taking norms on either side yields:
\begin{align}
  \bigg\| \vect{e}_i\big(t + \tau;&\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) -
  \overline{\vect{e}}_i\big(t + \tau;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) \bigg\| \\[2.5ex]
  &=\bigg\| \int_{t}^{t + \tau} g_i\big(\vect{e}_i(s;\ \vect{e}_i(t)), \overline{\vect{u}}_i^{\star}(s)\big) ds
     - \int_{t}^{t + \tau} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t)), \overline{\vect{u}}_i^{\star}(s)\big) ds
    + \int_{t}^{t + \tau}\delta_i(s)ds \bigg\| \\[2.5ex]
  & \leq \bigg\| \int_{t}^{t + \tau} g_i\big(\vect{e}_i(s;\ \vect{e}_i(t)), \overline{\vect{u}}_i^{\star}(s)\big) ds
     - \int_{t}^{t + \tau} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t)), \overline{\vect{u}}_i^{\star}(s)\big) ds \bigg\|
     + (t + \tau - t)\overline{\delta}_i \\[2.5ex]
  &=
     \int_{t}^{t + \tau} \bigg\| g_i\big(\vect{e}_i(s;\ \vect{e}_i(t)), \overline{\vect{u}}_i^{\star}(s)\big)s
     - g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t)), \overline{\vect{u}}_i^{\star}(s)\big) \bigg\| ds + \tau \overline{\delta}_i\\[2.5ex]
  &\leq L_{g_i} \int_{t}^{t + \tau} \bigg\| \vect{e}_i\big(s;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) -
  \overline{\vect{e}}_i\big(s;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) \bigg\| ds + \tau \overline{\delta}_i  \\[2.5ex]
\end{align}
since $g_i$ is Lipschitz continuous in $\mathcal{E}_i$ with Lipschitz constant
$L_{g_i}$. Reformulation yields
\begin{align}
  \bigg\| \vect{e}_i\big(t+\tau;&\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) -
  \overline{\vect{e}}_i\big(t+\tau;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) \bigg\| \\[2.5ex]
  &\leq \tau \overline{\delta}_i
  + L_{g_i} \int_{0}^{\tau} \bigg\| \vect{e}_i\big(t + s;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) -
  \overline{\vect{e}}_i\big(t + s;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) \bigg\| ds \\[2.5ex]
\end{align}
By applying the Gr\"{o}nwall-Bellman inequality we get:
\begin{align}
  \bigg\| \vect{e}_i\big(t + \tau;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) &-
    \overline{\vect{e}}_i\big(t + \tau;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big), \vect{e}_i(t)\big) \bigg\| \\[2.5ex]
  &\leq \tau \overline{\delta}_i +  L_{g_i} \int_{0}^{\tau} s \overline{\delta}_i e^{L_{g_i}(\tau - s)} ds \\[2.5ex]
  &= \tau \overline{\delta}_i - \overline{\delta}_i \int_{0}^{\tau} s  \big(e^{L_{g_i}(\tau - s)}\big)' ds \\[2.5ex]
  &= \tau \overline{\delta}_i -
    \overline{\delta}_i \bigg( \big[s e^{L_{g_i}(\tau - s)}\big]_0^{\tau}
      - \int_{0}^{\tau} e^{L_{g_i}(\tau - s)}ds\bigg) \\[2.5ex]
  &= \tau \overline{\delta}_i - \overline{\delta}_i \bigg( \tau + \dfrac{1}{L_{g_i}} (1- e^{L_{g_i}\tau})\bigg) \\[2.5ex]
  &= \dfrac{\overline{\delta}_i}{\L_{g_i}} (e^{L_{g_i}\tau} - 1)
\end{align}
\qedsymbol
\end{gg_box}

% --- END diff real_vs_predicted at t_(k+1) from t_k ---------------------------

\textbf{Feasibility analysis}
In this section we will show that there can be constructed an admissible
but not necessarily optimal control input according to definition
\eqref{definition:admissible_input}.
Consider a sampling instant $t_k$ for which a
solution $\overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big)$ to
problem \eqref{problem:opt_with_disturbances} exists.
%In between $t_k$ and $t_k + h$,
%where $h$ is the sampling time, the optimal control signal
%$\vect{u}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big)$ is applied to the open-loop
%system.
Suppose now a time instant $t_{k+1}$ such that\footnote{It is not strictly necessary
that $t_{k+1} = t_k + h$ here, however it is necessary for the following that
$t_{k+1} - t_k \leq h$} $t_k < t_{k+1} < t_k + T_p$, and consider that the
optimal control signal calculated at $t_k$ is comprised by the following two
portions:

\begin{equation}
  \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big) = \left\{
      \begin{array}{ll}
        \overline{\vect{u}}_i^{\star}\big(\tau_1;\ \vect{e}_i(t_k)\big), & \tau_1 \in [t_k, t_{k+1}] \\[2.5ex]
        \overline{\vect{u}}_i^{\star}\big(\tau_2;\ \vect{e}_i(t_k)\big), & \tau_2 \in [t_{k+1}, t_k + T_p]
      \end{array}
      \right.
  \label{eq:optimal_input_portions}
\end{equation}

Both portions are admissible since the calculated optimal control input is
admissible, and hence they both conform to the input constraints.
As for the resulting predicted states, they satisfy the state constraints, and,
crucially: $\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \in \Omega_i$.
Furthermore, according to assumption (3) of the theorem, there exists an
admissible (and certainly not guaranteed optimal) input
$\vect{h}_i \in \mathcal{U}_i$ that renders $\Psi_i$ invariant
over $[t_k + T_p, t_k + T_p + h]$.

Given the above facts, we can construct an admissible input
$\widetilde{\vect{u}}_i(\cdot)$  for time $t_{k+1}$ by sewing together the second
portion of \eqref{eq:optimal_input_portions} and the admissible input
$\vect{h}_i(\cdot)$:

\begin{equation}
  \widetilde{\vect{u}}_i(\tau) = \left\{
      \begin{array}{ll}
        \overline{\vect{u}}_i^{\star}\big(\tau;\ \vect{e}_i(t_k)\big), & \tau \in [t_{k+1}, t_k + T_p] \\[2.5ex]
        \vect{h}_i\big(\overline{\vect{e}}_i\big(\tau;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k))\big), & \tau \in (t_k + T_p, t_{k+1} + T_p]
      \end{array}
      \right.
\label{eq:optimal_input_t_plus_one}
\end{equation}

Applied at time $t_{k+1}$, $\widetilde{\vect{u}}_i(\tau)$
is an admissible control input with regard to the input constraints as
a composition of admissible control inputs, for
all $\tau \in [t_{k+1}, t_{k+1} + T_p]$.


%-------- START Feasibility point 2 --------------------------------------------
Furthermore, $\overline{\vect{e}}_i\big(t_{k+1} + s;\ \widetilde{\vect{u}}_i(\cdot), \vect{e}_i(t_{k+1})\big) \in \mathcal{E}_i \ominus \mathcal{B}_{s}$,
for all $s \in [0,  T_p]$


\begin{gg_box}

By applying lemma \eqref{lemma:diff_state_from_same_conditions} for
$t=t_{k+1} + s$ and $\tau=t_k$ we get
\begin{align}
    \bigg\|
      \vect{e}_i\big(t_{k+1}+s;\  \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)
      -\overline{\vect{e}}_i\big(t_{k+1} + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)
    \bigg\|
    \leq \dfrac{\overline{\delta}_i}{L_{g_i}}\big(e^{L_{g_i} (h+s)}-1\big)
\end{align}
or, in set language
\begin{align}
      \vect{e}_i\big(t_{k+1}+s;\  \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)
      -\overline{\vect{e}}_i\big(t_{k+1} + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)
      \in \mathcal{B}_{i, h+s}
\end{align}

By applying a reasoning identical to the proof of lemma
\eqref{lemma:diff_state_from_same_conditions} for $t=t_{k+1}$ (in the model
equation) and $t = t_k$ (in the real model equation), and $\tau = s$ we get
\begin{align}
    \bigg\|
      \vect{e}_i\big(t_{k+1}+s;\  \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)
      -\overline{\vect{e}}_i\big(t_{k+1} + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    \bigg\|
  \leq \dfrac{\overline{\delta}_i}{L_{g_i}}\big(e^{L_{g_i} s}-1\big)
\end{align}
which translates to
\begin{align}
      \vect{e}_i\big(t_{k+1}+s;\  \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)
      -\overline{\vect{e}}_i\big(t_{k+1} + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
      \in \mathcal{B}_{i,s}
\end{align}

Furthermore, we know that the solution to the optimization problem is
feasible at time $t_k$, which means that
\begin{align}
  \overline{\vect{e}}_i\big(t_{k+1} + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \in \mathcal{E}_i \ominus \mathcal{B}_{i,h+s}
\end{align}
Let us for sake of readability set
\begin{align}
  \vect{e}_{0} &= \vect{e}_i\big(t_{k+1} + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \\[2.5ex]
  \overline{\vect{e}}_{0} &= \overline{\vect{e}}_i\big(t_{k+1} + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \\[2.5ex]
  \overline{\vect{e}}_{1} &= \overline{\vect{e}}_i\big(t_{k+1} + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
\end{align}
and translate the above system of include statements to:
\begin{align}
  \vect{e}_{i,0} - \overline{\vect{e}}_{i,0} &\in \mathcal{B}_{i, h+s} \\[2.5ex]
  \vect{e}_{i,0} - \overline{\vect{e}}_{i,1} &\in \mathcal{B}_{i,s} \\[2.5ex]
  \overline{\vect{e}}_{i,0} &\in \mathcal{E}_i \ominus \mathcal{B}_{i,h+s}
\end{align}
First we will focus on the two first statements, and we will derive a result
that will combine with the third statement so as to prove that the predicted
state will be feasible from $t_{k+1}$ to $t_{k+1} + T_p$. Subtracting the
second from the first yields
\begin{align}
  \overline{\vect{e}}_{i,1} - \overline{\vect{e}}_{i,0} \in \mathcal{B}_{i, h+s} \ominus \mathcal{B}_{i,s}
\end{align}
Now we introduce the third statement
\begin{align}
  \overline{\vect{e}}_{i,0} &\in \mathcal{E}_i \ominus \mathcal{B}_{i,h+s} \\[2.5ex]
  \overline{\vect{e}}_{i,1}- \overline{\vect{e}}_{i,0} &\in \mathcal{B}_{i, h+s} \ominus \mathcal{B}_{i,s}
\end{align}
Adding the latter to the former yields
\begin{align}
  \overline{\vect{e}}_{i,1} \in \big(\mathcal{E}_i \ominus \mathcal{B}_{i,h+s}\big) \oplus \big(\mathcal{B}_{i, h+s} \ominus \mathcal{B}_{i,s}\big)
\end{align}
But\footnote{
  Suppose sets $A,B,C$ and vectors
  $\vect{a} \in A, \vect{b} \in B, \vect{c} \in C$, where
  $\vect{a}, \vect{b}, \vect{c} \in \mathbb{R}^n$. Then
\begin{align}
  A \ominus B = \{\vect{a} - \vect{b},\ \vect{a} \in A, \vect{b} \in B\} \\[2.5ex]
  B \ominus C = \{\vect{b} - \vect{c},\ \vect{b} \in B, \vect{c} \in C\}
\end{align}
Adding the latter to the former yields
\begin{align}
  (A \ominus B) \oplus (B \ominus C)
    &= \{\vect{a} - \vect{b} + \vect{b} - \vect{c},\ \vect{a} \in A, \vect{b} \in B, \vect{c} \in C\} \\[2.5ex]
    &= \{\vect{a} - \vect{c},\ \vect{a} \in A, \vect{c} \in C\}
\end{align}
On the other hand
\begin{align}
  A \oplus B = \{\vect{a} + \vect{b},\ \vect{a} \in A, \vect{b} \in B\} \\[2.5ex]
  B \oplus C = \{\vect{b} + \vect{c},\ \vect{b} \in B, \vect{c} \in C\}
\end{align}
Subtracting the latter from the former yields
\begin{align}
  (A \oplus B) \ominus (B \oplus C) &= \{\vect{a} + \vect{b} - \vect{b} - \vect{c},\ \vect{a} \in A, \vect{b} \in B, \vect{c} \in C\} \\[2.5ex]
    &= \{\vect{a} - \vect{c},\ \vect{a} \in A, \vect{c} \in C\}
\end{align}
Therefore
\begin{align}
  (A \ominus B) \oplus (B \ominus C) = (A \oplus B) \ominus (B \oplus C)
\end{align}
}
$(A \ominus B) \oplus (B \ominus C) = (A \oplus B) \ominus (B \oplus C)$
for arbitrary sets $A,B,C$. Hence
\begin{align}
  \overline{\vect{e}}_{i,1} \in \big(\mathcal{E}_i \oplus \mathcal{B}_{i,h+s}\big) \ominus \big(\mathcal{B}_{i, h+s} \oplus \mathcal{B}_{i,s}\big)
\end{align}
Using implication\footnote{
$A = B_1 \oplus B_2 \Rightarrow A \ominus B = (A \ominus B_1) \ominus B_2$}
(v) of theorem 2.1 from \cite{kolmanovsky} yields
\begin{align}
  \overline{\vect{e}}_{i,1} \in \bigg(\big(\mathcal{E}_i \oplus \mathcal{B}_{i,h+s}\big) \ominus \mathcal{B}_{i, h+s}\bigg) \ominus \mathcal{B}_{i,s}
\end{align}
Using implication\footnote{$(A \oplus B) \ominus B \subset A$}
(3.1.11) from \cite{schneider_2013} yields
\begin{align}
  \overline{\vect{e}}_{i,1} \in \mathcal{E}_i \ominus \mathcal{B}_{i,s}
\end{align}
Translating back to our native language, we get the desired result:
\begin{align}
  \overline{\vect{e}}_i\big(t_{k+1} + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big) \in \mathcal{E}_i \ominus \mathcal{B}_{i,s}
\end{align}
\qedsymbol
\end{gg_box}
%---------- END Feasibility point 2 --------------------------------------------

%-------- START Feasibility point 3 --------------------------------------------
Finally, $\overline{\vect{e}}_i\big(t_{k+1} + T_p;\ \widetilde{\vect{u}}_i(\cdot), \vect{e}_i(t_{k+1}) \big) \in \Omega_i$.
\begin{gg_box}
To prove this statement we begin with
\begin{align}
  V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;&\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)\big)
    - V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \\[2.5ex]
  &\leq \bigg|  V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)\big)
    - V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg | \\[2.5ex]
  &\leq L_{V_i}\bigg \| \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    - \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k) \big)\Big\| \\[2.5ex]
  &=  L_{V_i}\bigg \|  \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    - \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k) \big)\Big\|
\label{eq:from_DV_to_De}
\end{align}
Consulting with remark \eqref{remark:predicted_actual_equations_with_disturbance}
we get that the two terms interior to the norm are respectively equal to
\begin{alignat}{2}
  \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    &= \vect{e}_i(t_{k+1}) &&+ \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_{k+1})), \overline{\vect{u}}_i^{\star}(s) \big)ds \\[2.5ex]
    &\text{and}\\[2.5ex]
  \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)
    &= \vect{e}_i(t_k)     &&+ \int_{t_k}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s) \big)ds \\[2.5ex]
    &= \vect{e}_i(t_k)     &&+ \int_{t_k}^{t_{k+1}} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s) \big)ds \\[2.5ex]
    &                      &&+ \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s) \big)ds \\[2.5ex]
    &= \overline{\vect{e}}_i(t_{k+1}) &&+ \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s) \big)ds \\[2.5ex]
\end{alignat}
Subtracting the latter from the former and taking norms on either side we get
\begin{alignat}{2}
  &\bigg \|\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    - \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\| \\[2.5ex]
  &= \bigg \| \vect{e}_i(t_{k+1}) - \overline{\vect{e}}_i(t_{k+1})
  + \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_{k+1})), \overline{\vect{u}}_i^{\star}(s) \big)ds
    - \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s) \big)ds \bigg \| \\[2.5ex]
  &\leq \bigg \| \vect{e}_i(t_{k+1}) - \overline{\vect{e}}_i(t_{k+1}) \bigg\| \\[2.5ex]
  &+ \bigg \| \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_{k+1})), \overline{\vect{u}}_i^{\star}(s) \big)ds
    - \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s) \big)ds \bigg \| \\[2.5ex]
  &= \bigg \| \vect{e}_i(t_{k+1}) - \overline{\vect{e}}_i(t_{k+1}) \bigg\|
  + \bigg \| \int_{t_{k+1}}^{t_k + T_p} \bigg( g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_{k+1})), \overline{\vect{u}}_i^{\star}(s) \big)
  -  g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s) \big) \bigg) ds \bigg \| \\[2.5ex]
  &= \bigg \| \vect{e}_i(t_{k+1}) - \overline{\vect{e}}_i(t_{k+1}) \bigg\|
  +  \int_{t_{k+1}}^{t_k + T_p} \bigg\| g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_{k+1})), \overline{\vect{u}}_i^{\star}(s) \big)
  -  g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s) \big) \bigg\| ds \\[2.5ex]
  &\leq \bigg \| \vect{e}_i(t_{k+1}) - \overline{\vect{e}}_i(t_{k+1}) \bigg\|
  +  L_{g_i} \int_{t_{k+1}}^{t_k + T_p} \bigg\| \overline{\vect{e}}_i\big(s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
  - \overline{\vect{e}}_i\big(s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\| ds \\[2.5ex]
  &= \bigg \| \vect{e}_i(t_{k+1}) - \overline{\vect{e}}_i(t_{k+1}) \bigg\|
  +  L_{g_i} \int_{h}^{T_p} \bigg\| \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
  - \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\| ds
\end{alignat}
By applying the  Gr\"{o}nwall-Bellman inequality we get
\begin{alignat}{2}
  \bigg \|\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    - \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\|
    \leq \bigg \| \vect{e}_i(t_{k+1}) - \overline{\vect{e}}_i(t_{k+1}) \bigg\| e^{L_{g_i} (T_p - h)}
\end{alignat}
By applying lemma \eqref{lemma:diff_state_from_same_conditions} for $t = t_k$ and
$\tau = h$ we get
\begin{alignat}{2}
  \bigg \|\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    - \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\|
    \leq \dfrac{\overline{\delta}_i}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)}
\end{alignat}

Hence \eqref{eq:from_DV_to_De} becomes
\begin{align}
  V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)\big)
    &- V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \\[2.5ex]
  &\leq L_{V_i}\bigg \| \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    - \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k) \big)\Big\| \\[2.5ex]
  &= L_{V_i} \dfrac{\overline{\delta}_i}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)}
\label{eq:from_DV_to_eq}
\end{align}

Since the solution to the optimization problem is assumed to be feasible
at time $t_k$, all states abide by their respective constraints, and in
particular, the predicted state
$\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k) \in \Omega_i$.
This means that
$V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \leq \varepsilon_{\Omega_i}$.
Hence \eqref{eq:from_DV_to_eq} becomes
\begin{align}
  V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)\big)
  &\leq V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) + L_{V_i} \dfrac{\overline{\delta}_i}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)} \\[2.5ex]
  &\leq \varepsilon_{\Omega_i} + L_{V_i} \dfrac{\overline{\delta}_i}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)}
\label{eq:from_DV_to_eq_2}
\end{align}

From assumption 5 of theorem \eqref{theorem:with_disturbances},
the upper bound of the disturbance is in turn bounded by
\begin{align}
  \overline{\delta}_i \leq \dfrac{\varepsilon_{\Psi_i} - \varepsilon_{\Omega_i}}{\dfrac{L_{V_i}}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)}}
\end{align}
Therefore
\begin{align}
  V_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)\big)
  \leq  \varepsilon_{\Omega_i} - \varepsilon_{\Omega_i} + \varepsilon_{\Psi_i} = \varepsilon_{\Psi_i}
\end{align}
which means that the state
$\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big) \in \Psi_i$.
From assumption 2 of theorem \eqref{theorem:with_disturbances},
and since $\Psi_i \subseteq \Phi_i$, there is an admissible control signal
$\vect{h}_i\big(\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)\big)$
such that
\begin{align}
  \overline{\vect{e}}_i\big(t_{k+1} + T_p;\ \vect{h}_i(\cdot), \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)\big) \in \Omega_i
\end{align}
Therefore, overall
\begin{align}
  \overline{\vect{e}}_i\big(t_{k+1} + T_p;\ \widetilde{\vect{u}}_i(\cdot), \vect{e}_i(t_{k+1}) \big) \in \Omega_i
\end{align}
\qedsymbol

\begin{remark}
  Since $\Omega_i \subseteq \Psi$, $\Psi$ is a robust positively-invariant
  set according to definition \eqref{def:robust_positively_invariant_set},
  as also implied by assumption 3 of theorem \eqref{theorem:with_disturbances}.
  Therefore, $\Omega_i$ is also a robust positively invariant set.
\end{remark}
\end{gg_box}
%---------- END Feasibility point 3 --------------------------------------------

Piecing the two last statements together, and consulting with property
\eqref{property:restricted_constraint_set}, we conclude that the application of
the control input $\widetilde{\vect{u}}_i(\cdot)$ at time $t_{k+1}$ results in
the states of the real system abiding by their intended state constraints
across the entire horizon $[t_{k+1}, t_{k+1} + T_p]$.

Therefore, overall, the (sub-optimal) control input $\widetilde{\vect{u}}_i(\cdot)$
is admissible at time $t_{k+1}$ according to definition
\eqref{definition:admissible_input}, which means that feasibility of a solution
to the optimization problem at time $t_k$ implies feasibility at time
$t_{k+1} > t_k$, and, thus, since at time $t=0$ a solution is assumed to be
feasible, a solution to the optimal control problem is feasible for
all $t \geq 0$.\\[2.5ex]

\textbf{Convergence analysis}
The second part of the proof involves demonstrating the convergence of the
state $\vect{e}_i$ to the terminal set $\Omega_i$. In order for this
to be proved, it must be shown that a proper value function decreases along
the solution trajectories starting at some initial time $t_k$. We consider the
\textit{optimal} cost $J_i^{\star}\big(\vect{e}_i(t)\big)$ as a candidate
Lyapunov function:
$$J_i^{\star}\big(\vect{e}_i(t)\big) \triangleq J_i \Big(\vect{e}_i(t), \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big)\Big)$$
and, in particular, our goal is to show that that this cost decreases over
consecutive sampling instants $t_{k+1} = t_k + h$, i.e.
$J_i^{\star}\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) \leq 0$.\\[2.5ex]

In order not to wreak notational havoc, let us define the following terms:
\begin{gg_box}
\begin{itemize}
  \item $\vect{u}_{0,i}(\tau) \triangleq \overline{\vect{u}}_i^{\star}\big(\tau;\ \vect{e}_i(t_k)\big)$
    as the \textit{optimal} input that results from the solution to problem
    \eqref{problem:opt_without_disturbances} based on the measurement of state
    $\vect{e}_i(t_k)$, applied at time $\tau \geq t_k$
  \item $\vect{e}_{0,i}(\tau) \triangleq \overline{\vect{e}}_i\big(\tau;\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big), \vect{e}_i(t_k)\big)$
    as the \textit{predicted} state at time $\tau \geq t_k$, that is,
    the state that results from the application of the above input
    $\overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big)$ to the
    state $\vect{e}_i(t_k)$, at time $\tau$
  \item $\vect{u}_{1,i}(\tau) \triangleq \widetilde{\vect{u}}_i(\tau)$
    as the \textit{admissible} input at $\tau \geq t_{k+1}$ (see eq. \eqref{eq:optimal_input_t_plus_one})
  \item $\vect{e}_{1,i}(\tau) \triangleq \overline{\vect{e}}_i\big(\tau;\ \widetilde{\vect{u}}_i(\cdot), \vect{e}_i(t_{k+1})\big)$
    as the \textit{predicted} state at time $\tau \geq t_{k+1}$, that is,
    the state that results from the application of the above input
    $\widetilde{\vect{u}}_i(\cdot)$ to the state
    $\vect{e}_i\big(t_{k+1};\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big), \vect{e}_i(t_k)\big)$, at time $\tau$
\end{itemize}
\end{gg_box}





Before beginning to prove convergence, it is worth noting that while the cost
$$J_i \Big(\vect{e}_i(t), \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big)\Big)$$
is optimal (in the sense that it is based on the optimal input, which provides
its minimum realization), a cost that is based on a plainly admissible
(and thus, without loss of generality, sub-optimal) input
$\vect{u}_i \not= \overline{\vect{u}}_i^{\star}$ will result in a configuration where
\begin{equation}
J_i \Big(\vect{e}_i(t), \vect{u}_i\big(\cdot;\ \vect{e}_i(t)\big)\Big)
\geq J_i \Big(\vect{e}_i(t), \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big)\Big)
\end{equation}

Let us now begin our investigation on the sign of the difference between the cost
that results from the application of the feasible input $\vect{u}_{1,i}$,
which we shall denote by $\overline{J}_i\big(\vect{e}_i(t_{k+1})\big)$,
and the optimal cost $J_i^{\star}\big(\vect{e}_i(t_k)\big)$, while reminding
ourselves that
$J_i \big(\vect{e}_i(t), \overline{\vect{u}}_i (\cdot)\big)$ $=$
$\int_{t}^{t + T_p} F_i \big(\overline{\vect{e}}_i(s), \overline{\vect{u}}_i (s)\big) ds$ $+$
$V_i \big(\overline{\vect{e}}_i (t + T_p)\big)$:
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) =\
   & V_i \big(\vect{e}_{1,i} (t_{k+1} + T_p)\big) + \int_{t_{k+1}}^{t_{k+1} + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) ds \\[2.5ex]
  -&V_i \big(\vect{e}_{0,i} (t_k + T_p)\big) - \int_{t_k}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) ds
\end{align}
Considering that $t_k < t_{k+1} < t_k + T_p < t_{k+1} + T_p$, we break down the
two integrals above in between these intervals:
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) &= \\[2.5ex]
    V_i \big(\vect{e}_{1,i} (t_{k+1} + T_p)\big)
    &+ \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s \\[2.5ex]
    -V_i \big(\vect{e}_{0,i} (t_k + T_p)\big)
    &- \int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
    - \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
\label{eq:convergence_4_integrals_2}
\end{align}



We begin working on \eqref{eq:convergence_4_integrals_2} focusing
first on the difference between the two intervals over $[t_{k+1}, t_{k+1} + T_p]$:
\begin{align}
  \int_{t_{k+1}}^{t_k + T_p} F_i \big(&\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s
  - \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s \\[2.5ex]
  &= \int_{t_k+h}^{t_k + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s
    - \int_{t_k+h}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s \\[2.5ex]
  & \leq \bigg| \int_{t_k+h}^{t_k + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s
    - \int_{t_k+h}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s \bigg| \\[2.5ex]
  & = \bigg| \int_{t_k+h}^{t_k + T_p} \bigg( F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big)
    -  F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) \bigg) d s \bigg| \\[2.5ex]
  & = \int_{t_k+h}^{t_k + T_p} \bigg| F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big)
    -  F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) \bigg| d s \\[2.5ex]
  & \leq L_{F_i}\int_{t_k+h}^{t_k + T_p} \bigg\| \overline{\vect{e}}_i\big(s;\ \vect{u}_{1,i} (\cdot), \vect{e}_i(t_k + h) \big)
    -  \overline{\vect{e}}_i\big(s;\ \vect{u}_{0,i} (\cdot), \vect{e}_i(t_k)\big) \bigg\| d s \\[2.5ex]
    & = L_{F_i}\int_{h}^{T_p} \bigg\| \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k + h) \big)
    -  \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\| d s
\label{eq:integrals_over_same_u_LV}
\end{align}


\begin{gg_box}
Consulting with remark \eqref{remark:predicted_actual_equations_with_disturbance}
for the two different initial conditions we get
\begin{alignat}{2}
  \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k +h)\big)
    &= \vect{e}_i(t_k +h)
    &&+ \int_{t_k +h}^{t_k + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k + h)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau
\end{alignat}
and
\begin{alignat}{2}
  \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)
    &= \vect{e}_i(t_k)
    &&+ \int_{t_k}^{t_k + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
  &= \vect{e}_i(t_k)
    &&+ \int_{t_k}^{t_k + h} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
  & &&+ \int_{t_k + h}^{t_k + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau
\end{alignat}
Subtracting the latter from the former and taking norms on either side yields
\begin{align}
  &\bigg \| \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k +h)\big)
    -\overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg \| \\[2.5ex]
    &= \bigg\| \vect{e}_i(t_k +h)
 - \bigg( \vect{e}_i(t_k) + \int_{t_k}^{t_k + h} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \bigg) \\[2.5ex]
 &+ \int_{t_k +h}^{t_k + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k + h)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau
    - \int_{t_k + h}^{t_k + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \bigg\| \\[2.5ex]
    &= \bigg\| \vect{e}_i(t_k +h) - \overline{\vect{e}}_i(t_k + h)
    + \int_{t_k +h}^{t_k + s} \bigg( g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k + h)), \overline{\vect{u}}_i^{\star}(\tau)\big)
    -  g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) \bigg) d\tau \bigg\| \\[2.5ex]
    &\leq \bigg\| \vect{e}_i(t_k +h) - \overline{\vect{e}}_i(t_k + h) \bigg \|
    + \bigg\| \int_{t_k +h}^{t_k + s} \bigg( g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k + h)), \overline{\vect{u}}_i^{\star}(\tau)\big)
    -  g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) \bigg) d\tau \bigg\| \\[2.5ex]
    &= \bigg\| \vect{e}_i(t_k +h) - \overline{\vect{e}}_i(t_k + h) \bigg \|
    + \int_{t_k +h}^{t_k + s} \bigg\| g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k + h)), \overline{\vect{u}}_i^{\star}(\tau)\big)
    -  g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) \bigg\| d\tau \\[2.5ex]
    &\leq \bigg\| \vect{e}_i(t_k +h) - \overline{\vect{e}}_i(t_k + h) \bigg \|
    + L_{g_i} \int_{t_k +h}^{t_k + s} \bigg\| \overline{\vect{e}}_i\big(\tau;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k + h) \big)
    -  \overline{\vect{e}}_i\big(\tau;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\| d\tau \\[2.5ex]
    &= \bigg\| \vect{e}_i(t_k +h) - \overline{\vect{e}}_i(t_k + h) \bigg \|
    + L_{g_i} \int_{h}^{s} \bigg\| \overline{\vect{e}}_i\big(t_k + \tau;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k + h) \big)
    -  \overline{\vect{e}}_i\big(t_k + \tau;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\| d\tau \\[2.5ex]
    \label{eq:df_interim_es}
\end{align}

Given that from lemma \eqref{lemma:diff_state_from_same_conditions}
the first term of the sum featured in \eqref{eq:df_interim_es} is a constant,
by application of the the Gr\"{o}nwall-Bellman inequality,
\eqref{eq:df_interim_es} becomes:
\begin{align}
  \bigg \| \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k +h)\big)
    -\overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg \|
    &\leq \bigg\| \vect{e}_i(t_k +h) - \overline{\vect{e}}_i(t_k + h) \bigg \| e^{L_{g_i}(s-h)} \\[2.5ex]
    &\leq \dfrac{\overline{\delta}_i}{\L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i}(s-h)}
\end{align}
\end{gg_box}

Given the above result, \eqref{eq:integrals_over_same_u_LV} becomes
\begin{align}
  \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{1,i}(s),& \vect{u}_{1,i} (s)\big) d s
    - \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s \\[2.5ex]
  & \leq L_{F_i}\int_{h}^{T_p} \dfrac{\overline{\delta}_i}{\L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i}(s-h)} d s \\[2.5ex]
  &=  L_{F_i} \dfrac{\overline{\delta}_i}{\L_{g_i}} (e^{L_{g_i}h} - 1) \int_{h}^{T_p} e^{L_{g_i}(s-h)} d s \\[2.5ex]
  & = L_{F_i} \dfrac{\overline{\delta}_i}{\L_{g_i}} (e^{L_{g_i} h} - 1) \dfrac{1}{L_{g_i}}(e^{L_{g_i}(T_p-h)} - 1) \\[2.5ex]
  & = L_{F_i} \dfrac{\overline{\delta}_i}{\L_{g_i}^2} (e^{L_{g_i} h} - 1) (e^{L_{g_i}(T_p-h)} - 1)
\end{align}


Hence we discovered that
\begin{bw_box}
\begin{align}
  \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s
  &- \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s \\[2.5ex]
  & \leq L_{F_i} \dfrac{\overline{\delta}_i}{\L_{g_i}^2} (e^{L_{g_i} h} - 1) (e^{L_{g_i}(T_p-h)} - 1)
\label{eq:end_result_two_integrals}
\end{align}
\end{bw_box}

With this partial result established, we turn back to the remaining terms
found in \eqref{eq:convergence_4_integrals_2} and, in particular, we focus on
the integral
\begin{align}
  \int_{t_k + T_p}^{t_{k+1} + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s
\end{align}
\begin{gg_box}
  We discern that the range of the above integral has a length\footnote{$(t_{k+1} + T_p) - (t_k + T_p) = t_{k+1} - t_k = h$}
  equal to the length of the interval where assumption 2 of theorem
  \eqref{theorem:with_disturbances} holds.
  Integrating the expression found in the assumption over the
  interval $[t_k + T_p, t_{k+1} + T_p]$, for the controls and states applicable
  in it we get
  \begin{align}
    \int_{t_k + T_p}^{t_{k+1} + T_p} \Bigg(\dfrac{\partial V_i}{\partial \vect{e}_{1,i}} g_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big)
    + F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big)\Bigg) ds &\leq 0 \\[2.5ex]
    \int_{t_k + T_p}^{t_{k+1} + T_p} \dfrac{d}{ds} V_i\big(\vect{e}_{1,i}(s)\big) d s
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds &\leq 0 \\[2.5ex]
    V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big) - V_i\big(\vect{e}_{1,i}(t_k + T_p)\big)
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds &\leq 0 \\[2.5ex]
    V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds &\leq V_i\big(\vect{e}_{1,i}(t_k + T_p)\big)
  \end{align}

  The left-hand side expression is the same as the first two terms in the
  right-hand side of equality \eqref{eq:convergence_4_integrals_2}. We can
  introduce the third one by subtracting it from both sides:
  \begin{align}
    V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
    &+ \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
    - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \\[2.5ex]
    &\leq V_i\big(\vect{e}_{1,i}(t_k + T_p)\big)
    - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \\[2.5ex]
    &\leq \Big|V_i\big(\vect{e}_{1,i}(t_k + T_p)\big)
    - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big)\Big| \\[2.5ex]
    &\leq L_{V_i}\bigg \|\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    - \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k) \big)\Big\| \\[2.5ex]
    &\leq L_{V_i} \dfrac{\overline{\delta}_i}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)} \text{ (from \eqref{eq:from_DV_to_eq})}
  \end{align}
\end{gg_box}
Hence, we discovered that
\begin{bw_box}
\begin{align}
  V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
  + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
  &- V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \\[2.5ex]
  &\leq L_{V_i}\dfrac{\overline{\delta}_i}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)}
  \label{eq:end_result_diff_V_plus_int}
\end{align}
\end{bw_box}

Adding the milestone inequalities \eqref{eq:end_result_two_integrals} and
\eqref{eq:end_result_diff_V_plus_int} yields
\begin{alignat}{2}
  &\int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s
  - \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s \\[2.5ex]
  &+ V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
  + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
  - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \\[2.5ex]
  &\leq L_{F_i} \dfrac{\overline{\delta}_i}{\L_{g_i}^2} (e^{L_{g_i} h} - 1) (e^{L_{g_i}(T_p-h)} - 1)
  + L_{V_i}\dfrac{\overline{\delta}_i}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)}
\end{alignat}
and therefore \eqref{eq:convergence_4_integrals_2}, by bringing the integral
ranging from $t_k$ to $t_{k+1}$ to the left-hand side, becomes
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big)
    - J_i^{\star}\big(\vect{e}_i(t_k)\big)
    &+ \int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s \\[2.5ex]
    &\leq L_{F_i} \dfrac{\overline{\delta}_i}{\L_{g_i}^2} (e^{L_{g_i} h} - 1) (e^{L_{g_i}(T_p-h)} - 1)
  + L_{V_i}\dfrac{\overline{\delta}_i}{L_{g_i}} (e^{L_{g_i}h} - 1) e^{L_{g_i} (T_p - h)}
\end{align}

By rearranging terms, the cost difference becomes bounded by
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) &- J_i^{\star}\big(\vect{e}_i(t_k)\big) \\[2.5ex]
  &\leq -\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
    + \dfrac{\overline{\delta}_i}{L_{g_i}} \bigg(e^{L_{g_i}h} - 1\bigg)
    \bigg(\big(L_{V_i} + \dfrac{L_{F_i}}{\L_{g_i}}\big) e^{L_{g_i}(T_p-h)}  - \dfrac{L_{F_i}}{\L_{g_i}}\bigg) \\[2.5ex]
  &= \xi_i -\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
\end{align}
where
\begin{align}
  \xi_i = \dfrac{\overline{\delta}_i}{L_{g_i}} \bigg(e^{L_{g_i}h} - 1\bigg)
    \bigg(\big(L_{V_i} + \dfrac{L_{F_i}}{\L_{g_i}}\big) e^{L_{g_i}(T_p-h)}  - \dfrac{L_{F_i}}{\L_{g_i}}\bigg) > 0
\end{align}
is the contribution of the bounded additive disturbance $\delta_i(t)$
to the nominal cost difference (the case without disturbances).

\begin{gg_box}
  $F_i$ is a positive-definite function as a sum of a positive-definite
  $\|\vect{u}_i\|^2_{\mat{R}_i}$ and a positive semi-definite function
  $\|\vect{e}_i\|^2_{\mat{Q}_i}$. If we denote by
  $m_i = \lambda_{min}(\mat{Q}_i, \mat{R}_i) \geq 0$ the minimum eigenvalue
  between those of matrices $\mat{R}_i, \mat{Q}_i$, this means that
  \begin{align}
    F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) \geq m_i \|\vect{e}_{0,i}(s)\|^2
  \end{align}
  By integrating the above between our interval of interest $[t_k, t_{k+1}]$ we get
  \begin{align}
    \int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) &\geq \int_{t_k}^{t_{k+1}} m_i \|\vect{e}_{0,i}(s)\|^2 ds \\[2.5ex]
    \text{or}\\[2.5ex]
    -\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big)
    &\leq -m_i \int_{t_k}^{t_{k+1}} \| \overline{\vect{e}}_i(s;\ \overline{\vect{u}}_i^{\star}, \vect{e}_i(t_k)) \|^2 ds
  \end{align}
\end{gg_box}
This means that the cost difference is upper-bounded by
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big)
    &\leq \xi_i-m_i \int_{t_k}^{t_{k+1}} \| \overline{\vect{e}}_i(s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)) \|^2 ds
\end{align}
and since the cost $\overline{J}_i\big(\vect{e}_i(t_{k+1})\big)$ is, in general,
sub-optimal: $J_i^{\star}\big(\vect{e}_i(t_{k+1})\big) - \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) \leq 0$:
\begin{align}
 J_i^{\star}\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big)
   \leq \xi_i-m_i \int_{t_k}^{t_{k+1}} \| \overline{\vect{e}}_i(s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)) \|^2 ds
 \label{eq:J_opt_between_consecutive_k_2}
\end{align}

It is easy to verify that both right-hand side terms are class
$\mathcal{K}$ functions, and therefore, according to definition
\eqref{def:ISS_Lyapunov}, $J_i^{\star}\big(\vect{e}_i\big)$ is an ISS Lyapunov
function in $\Psi_i$. Given this fact, theorem
\eqref{def:ISS_Lyapunov_admit_theorem}, implies that the closed-loop
system is input-to-state stable in $\Psi_i$. Inevitably then, given assumptions
2 and 3 of theorem \eqref{theorem:with_disturbances}, the closed-loop
trajectories for the error state of agent $i \in \mathcal{V}$ converge to
the terminal set $\Omega_i$ and are trapped there.

In turn, this means that the system \eqref{eq:original_z_system} converges
to $\vect{z}_{i,des}$ while simultaneously conforming to
all constraints $\mathcal{Z}_i$, as $t \to \infty$. This conclusion holds
for all $i \in \mathcal{V}$, and hence, the compound system of agents
$\mathcal{V}$ is stable.
\qedsymbol


% NOT NEEDED -- THIS CALCULATES THE EXACT BOUND FOR ΔJ
%\textcolor{red}{THIS-THIS-THIS-THIS-THIS-THIS-THIS-THIS-THIS-THIS-THIS}
%\begin{bw_box}
  %It is possible that assumption 3b can be dispensed with.
  %If we make the assumption then by integration we conclude that
  %\begin{align}
    %&V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
    %+ \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
    %- V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \\[2.5ex]
    %&\leq L_{V_i}\bigg \|\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    %- \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k) \big)\Big\|
  %\end{align}
  %but the fact of the matter is that, since $V$ is Lipschitz continuous,
  %the above integral is irrelevant:
  %\begin{align}
    %&V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big) - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \\[2.5ex]
    %&\leq L_{V_i}\bigg \|\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})\big)
    %- \overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k) \big)\Big\|
  %\end{align}

  %Then, what is left to bound in the cost difference is the below difference:
  %\begin{align}
    %\int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
    %-\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
  %\end{align}
  %And it goes like this
  %\begin{align}
    %&\int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
      %-\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s \\[2.5ex]
    %&= \int_{0}^{h} F_i\big(\vect{e}_{1,i}(t_k + T_p + s), \vect{u}_{1,i}(t_k + T_p + s)\big) ds
      %-\int_{0}^{h} F_i \big(\vect{e}_{0,i}(t_k + s), \vect{u}_{0,i} (t_k + s)\big) d s \\[2.5ex]
    %&= \int_{0}^{h} \bigg( F_i\big(\vect{e}_{1,i}(t_k + T_p + s), \vect{u}_{1,i}(t_k + T_p + s)\big)
      %- F_i \big(\vect{e}_{0,i}(t_k + s), \vect{u}_{0,i} (t_k + s)\big) \bigg) d s \\[2.5ex]
    %&\leq \bigg\| \int_{0}^{h} \bigg( F_i\big(\vect{e}_{1,i}(t_k + T_p + s), \vect{u}_{1,i}(t_k + T_p + s)\big)
      %- F_i \big(\vect{e}_{0,i}(t_k + s), \vect{u}_{0,i} (t_k + s)\big) \bigg) d s \bigg \|\\[2.5ex]
    %&= \int_{0}^{h} \bigg\| F_i\big(\vect{e}_{1,i}(t_k + T_p + s), \vect{u}_{1,i}(t_k + T_p + s)\big)
      %- F_i \big(\vect{e}_{0,i}(t_k + s), \vect{u}_{0,i} (t_k + s)\big) \bigg\| d s \\[2.5ex]
    %&\leq L_{F_i} \int_{0}^{h} \bigg\| \overline{\vect{e}}_i\big(t_k + T_p + s;\ \vect{u}_{i,f}(\cdot), \vect{e}_i(t_{k+1})\big)
      %- \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\| d s \\[2.5ex]
  %\label{eq:delta_F}
  %\end{align}
  %The respective terms interior to the norm are equal to
  %\begin{alignat}{2}
    %\overline{\vect{e}}_i\big(t_k + T_p + s;\ \vect{u}_{i,f}(\cdot), \vect{e}_i(t_{k+1})\big)
      %& = \vect{e}_i(t_{k+1})
      %&& + \int_{t_{k+1}}^{t_k + T_p + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_{k+1})), \vect{u}_{i,f}(\tau)\big) d\tau \\[2.5ex]
      %& = \vect{e}_i(t_{k+1})
      %&& + \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_{k+1})), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
      %& && + \int_{t_k + T_p}^{t_k + T_p + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_{k+1})), \vect{u}_{i,f}(\tau)\big) d\tau \\[2.5ex]
    %\overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)
      %& = \vect{e}_i(t_k)
      %&& + \int_{t_k}^{t_k + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
    %& = \vect{e}_i(t_k)
        %&& + \int_{t_k}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
      %&  && - \int_{t_k + s}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau
  %\end{alignat}



  %Subtracting the latter from the former and taking norms on either side yields
  %\begin{alignat}{2}
    %&\overline{\vect{e}}_i\big(t_k + T_p + s;\ \vect{u}_{i,f}(\cdot), \vect{e}_i(t_{k+1})\big)
      %-\overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \\[2.5ex]
    %&= \vect{e}_i(t_{k+1}) - \vect{e}_i(t_k) \\[2.5ex]
    %&+ \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_{k+1})), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
    %&+ \int_{t_k + T_p}^{t_k + T_p + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_{k+1})), \vect{u}_{i,f}(\tau)\big) d\tau \\[2.5ex]
    %&+ \int_{t_k + s}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
    %&- \int_{t_k}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
    %&= \vect{e}_i(t_{k+1})+ \int_{t_{k+1}}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_{k+1})), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
    %&- \vect{e}_i(t_k) - \int_{t_k}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau\\[2.5ex]
    %&+ \int_{t_k + T_p}^{t_k + T_p + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_{k+1})), \vect{u}_{i,f}(\tau)\big) d\tau \\[2.5ex]
    %&+ \int_{t_k + s}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
    %&= \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) \\[2.5ex]
    %&- \vect{e}_i(t_k) - \int_{t_k}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau\\[2.5ex]
    %&+ \int_{t_k + T_p}^{t_k + T_p + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_{k+1})), \vect{u}_{i,f}(\tau)\big) d\tau \\[2.5ex]
    %&+ \int_{t_k}^{t_k + T_p} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
    %&- \int_{t_k}^{t_k + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
    %&= \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) - \vect{e}_i(t_k) \\[2.5ex]
    %&+ \int_{t_k + T_p}^{t_k + T_p + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_{k+1})), \vect{u}_{i,f}(\tau)\big) d\tau \\[2.5ex]
    %&- \int_{t_k}^{t_k + s} g_i\big(\overline{\vect{e}}_i(\tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(\tau)\big) d\tau \\[2.5ex]
    %&= \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) - \vect{e}_i(t_k) \\[2.5ex]
    %&+ \int_{0}^{s} g_i\big(\overline{\vect{e}}_i(t_k + T_p + \tau;\ \vect{e}_i(t_{k+1})), \vect{u}_{i,f}(t_k + T_p + \tau)\big) d\tau \\[2.5ex]
    %&- \int_{0}^{s} g_i\big(\overline{\vect{e}}_i(t_k + \tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(t_k + \tau)\big) d\tau \\[2.5ex]
    %&= \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) - \vect{e}_i(t_k) \\[2.5ex]
    %&+ \int_{0}^{s} \bigg(g_i\big(\overline{\vect{e}}_i(t_k + T_p + \tau;\ \vect{e}_i(t_{k+1})), \vect{u}_{i,f}(t_k + T_p + \tau)\big) d\tau
    %-  g_i\big(\overline{\vect{e}}_i(t_k + \tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(t_k + \tau)\big) \bigg) d\tau \\[2.5ex]
  %\end{alignat}
  %Taking norms on either side yields
  %\begin{alignat}{2}
    %&\bigg\| \overline{\vect{e}}_i\big(t_k + T_p + s;\ \vect{u}_{i,f}(\cdot), \vect{e}_i(t_{k+1})\big)
      %-\overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\|\\[2.5ex]
    %&\leq \bigg\| \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) - \vect{e}_i(t_k) \bigg\| \\[2.5ex]
    %&+ \int_{0}^{s} \bigg\|g_i\big(\overline{\vect{e}}_i(t_k + T_p + \tau;\ \vect{e}_i(t_{k+1})), \vect{u}_{i,f}(t_k + T_p + \tau)\big) d\tau
    %-  g_i\big(\overline{\vect{e}}_i(t_k + \tau;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(t_k + \tau)\big) \bigg\| d\tau \\[2.5ex]
  %\end{alignat}
  %But $g_i$ is Lipschitz continuous in $\mathcal{E}$, hence
  %\begin{alignat}{2}
    %&\bigg\| \overline{\vect{e}}_i\big(t_k + T_p + s;\ \vect{u}_{i,f}(\cdot), \vect{e}_i(t_{k+1})\big)
      %-\overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\|\\[2.5ex]
    %&\leq \bigg\| \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) - \vect{e}_i(t_k) \bigg\| \\[2.5ex]
    %&+ L_{g_i} \int_{0}^{s} \bigg\|\overline{\vect{e}}_i\big(t_k + T_p + \tau;\ \vect{u}_{i,f}(\cdot), \vect{e}_i(t_{k+1})\big)
      %-\overline{\vect{e}}_i\big(t_k + \tau;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big)\| d\tau \\[2.5ex]
  %\end{alignat}
  %Considering the Gr\"{o}nwall-Bellman inequality we get
  %\begin{alignat}{2}
    %&\bigg\| \overline{\vect{e}}_i\big(t_k + T_p + s;\ \vect{u}_{i,f}(\cdot), \vect{e}_i(t_{k+1})\big)
      %-\overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\|\\[2.5ex]
    %&\leq \bigg\| \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) - \vect{e}_i(t_k) \bigg\| e^{L_{g_i}s}
  %\end{alignat}
  %Hence the original difference \eqref{eq:delta_F} becomes
  %\begin{align}
    %&\int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
      %-\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s \\[2.5ex]
    %&\leq L_{F_i} \int_{0}^{h} \bigg\| \overline{\vect{e}}_i\big(t_k + T_p + s;\ \vect{u}_{i,f}(\cdot), \vect{e}_i(t_{k+1})\big)
      %- \overline{\vect{e}}_i\big(t_k + s;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \bigg\| d s \\[2.5ex]
    %&\leq L_{F_i}\bigg\| \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) - \vect{e}_i(t_k) \bigg\|
      %\int_0^h e^{L_{g_i}s} ds \\[2.5ex]
    %&= \dfrac{L_{F_i}}{L_{g_i}}\bigg\| \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) - \vect{e}_i(t_k) \bigg\|
      %\big(e^{L_{g_i}h}-1\big)
  %\label{eq:delta_F_mid_result}
  %\end{align}
  %With this mid-result established, we need to locate the upper bound of the
  %norm in the above inequality.
  %\begin{align}
    %\bigg\| \overline{\vect{e}}_i(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_{k+1})) - \vect{e}_i(t_k) \bigg\|
    %= \bigg\| \vect{e}_i(t_{k+1})
  %+\int_{t_{k+1}}^{t_k + T_p} g_i \big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_{k+1})), \vect{u}_i^{\star}(s)\big)ds
    %- \vect{e}_i(t_k) \bigg\|
  %\end{align}
%\end{bw_box}
