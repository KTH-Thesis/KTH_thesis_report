%-------------------------------------------------------------------------------
\subsection{Feasibility and Convergence}

Under these considerations, we can now state the theorem that relates to
the guaranteeing of the stability of the compound system of agents
$i \in \mathcal{V}$, when each of them is assigned a desired
position which results in feasible displacements:\\

\begin{bw_box}
\begin{theorem}

  Suppose that

  \begin{enumerate}
    \item the terminal region $\mathcal{E}_{i,f} \subseteq \mathcal{E}_i$ is
      closed with $\vect{0} \in \mathcal{E}_{i,f}$
    \item a solution to the optimal control problem \eqref{position_based_cost}
      is feasible at time $t=0$, that is, assumptions
      \eqref{ass:measurements_access}, \eqref{ass:initial_conditions}, and
      \eqref{ass:intra_environmental_arrangement} hold at time $t=0$
    \item there exists an admissible control input
      $\vect{u}_{i,f} : [0, h] \to \mathcal{U}_i$ such that for all
      $\vect{e}_i \in \mathcal{E}_{i,f}$ and $\forall \tau \in [0,h]$:

      \begin{enumerate}
        \item $\vect{e}_i(\tau) \in \mathcal{E}_{i,f}$
        \item $\dfrac{\partial V_i}{\partial \vect{e}_i} g_i\big(\vect{e}_i(\tau), \vect{u}_{i,f}(\tau)\big)
          + F_i\big(\vect{e}_i(\tau), \vect{u}_{i,f}(\tau)\big) \leq 0$
      \end{enumerate}

  \end{enumerate}

  then the closed loop system \eqref{eq:position_based_error_model} under the
  control input \eqref{eq:position_based_optimal_u} converges to the set
  $\mathcal{E}_{i,f}$ when $t \to \infty$.

\end{theorem}
\end{bw_box}

\textbf{Proof}. The proof of the above theorem consists of two parts:
in the first, recursive feasibility is established, that is, initial
feasibility is shown to imply subsequent feasibility; in the second, and based
on the first part, it is shown that the error $\vect{e}_i(t)$ converges to the
terminal set $\mathcal{E}_{i,f}$.\\

\textbf{Feasibility analysis}
Consider a sampling instant $t_k$ for which a
solution $\overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big)$ to
\eqref{position_based_cost} exists.
%In between $t_k$ and $t_k + h$,
%where $h$ is the sampling time, the optimal control signal
%$\vect{u}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big)$ is applied to the open-loop
%system.
Suppose now a time instant $t_{k+1}$ such that\footnote{It is not strictly necessary
that $t_{k+1} = t_k + h$ here, however it is necessary for the following that
$t_{k+1} - t_k \leq h$} $t_k < t_{k+1} < t_k + T_p$, and consider that the
optimal control signal calculated at $t_k$ is comprised by the following two
portions:

\begin{equation}
  \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big) = \left\{
      \begin{array}{ll}
        \overline{\vect{u}}_i^{\star}\big(\tau_1;\ \vect{e}_i(t_k)\big), & \tau_1 \in [t_k, t_{k+1}] \\
        \overline{\vect{u}}_i^{\star}\big(\tau_2;\ \vect{e}_i(t_k)\big), & \tau_2 \in [t_{k+1}, t_k + T_p]
      \end{array}
      \right.
  \label{eq:optimal_input_portions}
\end{equation}

Both portions are admissible since the calculated optimal control input is
admissible, and hence they both conform to the input constraints.
As for the resulting predicted states, they satisfy the state constraints, and,
crucially: $\overline{\vect{e}}_i\big(t_k + T_p;\ \overline{\vect{u}}_i^{\star}(\cdot), \vect{e}_i(t_k)\big) \in \mathcal{E}_{i,f}$.
Furthermore, according to assumption (3) of the theorem, there exists an
admissible (and certainly not guaranteed optimal) input $\vect{u}_{i,f}$ that
renders $\mathcal{E}_{i,f}$ invariant over $[t_k + T_p, t_k + T_p + h]$.

Given the above facts, we can construct an admissible input
$\widetilde{\vect{u}}_i(\cdot)$  for time $t_{k+1}$ by sewing together the second
portion of \eqref{eq:optimal_input_portions} and the input
$\vect{u}_{i,f}(\cdot)$:

\begin{equation}
  \widetilde{\vect{u}}_i(\tau) = \left\{
      \begin{array}{ll}
        \overline{\vect{u}}_i^{\star}\big(\tau;\ \vect{e}_i(t_k)\big), & \tau \in [t_{k+1}, t_k + T_p] \\
        %\vect{u}_{i,f}\Big(\overline{\vect{e}}_i\big(t_k + T_p;\ \vect{u}_i^{\star}, \vect{e}_i(t_k)\big)\Big), & \tau_2 \in (t_k + T_p, t_{k+1} + T_p]
        \vect{u}_{i,f}(\tau - t_k - T_p), & \tau \in (t_k + T_p, t_{k+1} + T_p]
      \end{array}
      \right.
\label{eq:optimal_input_t_plus_one}
\end{equation}

Applied at time $t_{k+1}$, $\widetilde{\vect{u}}_i(\cdot)$
is an admissible control input as a composition of admissible control inputs.

This means that feasibility of a solution to the optimization problem at time
$t_k$ implies feasibility at time $t_{k+1} > t_k$, and, thus, since at time $t=0$
a solution is assumed to be feasible, a solution to the optimal control problem
is feasible for all $t \geq 0$.\\

\textbf{Convergence analysis}
The second part of the proof involves demonstrating the convergence of the
state $\vect{e}_i$ to the terminal set $\mathcal{E}_{i,f}$. In order for this
to be proved, it must be shown that a proper value function decreases along
the solution trajectories starting at some initial time $t_k$. We consider the
\textit{optimal} cost $J_i^{\star}\big(\vect{e}_i(t)\big)$ as a candidate
Lyapunov function:
$$J_i^{\star}\big(\vect{e}_i(t)\big) \triangleq J_i \Big(\vect{e}_i(t), \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big)\Big)$$
and, in particular, our goal is to show that that this cost decreases over
consecutive sampling instants $t_{k+1} = t_k + h$, i.e.
$J_i^{\star}\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) \leq 0$.\\

In order not to wreak notational havoc, let us define the following terms:
\begin{gg_box}
\begin{itemize}
  \item $\vect{u}_{0,i}(\tau) \triangleq \overline{\vect{u}}_i^{\star}\big(\tau;\ \vect{e}_i(t_k)\big)$
    as the \textit{optimal} input that results from the solution to problem
    \eqref{problem:opt_without_disturbances} based on the measurement of state
    $\vect{e}_i(t_k)$, applied at time $\tau \geq t_k$
  \item $\vect{e}_{0,i}(\tau) \triangleq \overline{\vect{e}}_i\big(\tau;\ \overline{\vect{u}}_i^{\star}\big(\tau;\ \vect{e}_i(t_k)\big), \vect{e}_i(t_k)\big)$
    as the \textit{predicted} state at time $\tau \geq t_k$, that is,
    the state that results from the application of the above input
    $\overline{\vect{u}}_i^{\star}\big(\tau;\ \vect{e}_i(t_k)\big)$ to the
    state $\vect{e}_i(t_k)$, at time $\tau$
  \item $\vect{u}_{1,i}(\tau) \triangleq \widetilde{\vect{u}}_i(\tau)$
    as the \textit{admissible} input at $\tau \geq t_{k+1}$ (see eq. \eqref{eq:optimal_input_t_plus_one})
  \item $\vect{e}_{1,i}(\tau) \triangleq \overline{\vect{e}}_i\big(\tau;\ \widetilde{\vect{u}}_i(\tau), \vect{e}_i(t_{k+1})\big)$
    as the \textit{predicted} state at time $\tau \geq t_{k+1}$, that is,
    the state that results from the application of the above input
    $\widetilde{\vect{u}}_i(\tau)$ to the state
    $\vect{e}_i\big(t_{k+1};\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big), \vect{e}_i(t_k)\big)$, at time $\tau$
\end{itemize}
\end{gg_box}

\begin{bw_box}
  \begin{remark}
    Given that no model mismatch or disturbances exist, for
    the predicted and actual states at time $\tau_1 \geq \tau_0$ it holds that:
    \begin{align}
      \vect{e}_i\big(\tau_1;\ \vect{u}_i(\cdot), \vect{e}_i(\tau_0)\big) &=
        \vect{e}_i(\tau_0) + \int_{\tau_0}^{\tau_1} g_i\big(\vect{e}_i(s;\ \vect{e}_i(\tau_0)), \vect{u}_i(s)\big) ds \\
      \overline{\vect{e}}_i\big(\tau_1;\ \vect{u}_i(\cdot), \vect{e}_i(\tau_0)\big) &=
        \vect{e}_i(\tau_0) + \int_{\tau_0}^{\tau_1} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(\tau_0)), \vect{u}_i(s)\big) ds
    \end{align}
    \label{remark:predicted_actual_equations_without_disturbance}
  \end{remark}
\end{bw_box}

Before beginning to prove convergence, it is worth noting that while the cost
$$J_i \Big(\vect{e}_i(t), \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big)\Big)$$
is optimal (in the sense that it is based on the optimal input, which provides
its minimum realization), a cost that is based on a plainly admissible
(and thus, without loss of generality, sub-optimal) input
$\vect{u}_i \not= \overline{\vect{u}}_i^{\star}$ will result in a configuration where
\begin{equation}
J_i \Big(\vect{e}_i(t), \vect{u}_i\big(\cdot;\ \vect{e}_i(t)\big)\Big)
\geq J_i \Big(\vect{e}_i(t), \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t)\big)\Big)
\end{equation}

Let us now begin our investigation on the sign of the difference between the cost
that results from the application of the feasible input $\vect{u}_{1,i}$,
which we shall denote by $\overline{J}_i\big(\vect{e}_i(t_{k+1})\big)$,
and the optimal cost $J_i^{\star}\big(\vect{e}_i(t_k)\big)$, while reminding
ourselves that
$J_i \big(\vect{e}_i(t), \overline{\vect{u}}_i (\cdot)\big)$ $=$
$\int_{t}^{t + T_p} F_i \big(\overline{\vect{e}}_i(s), \overline{\vect{u}}_i (s)\big) ds$ $+$
$V_i \big(\overline{\vect{e}}_i (t + T_p)\big)$:
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) =\
   & V_i \big(\vect{e}_{1,i} (t_{k+1} + T_p)\big) + \int_{t_{k+1}}^{t_{k+1} + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) ds \\
  -&V_i \big(\vect{e}_{0,i} (t_k + T_p)\big) - \int_{t_k}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) ds
\end{align}

Considering that $t_k < t_{k+1} < t_k + T_p < t_{k+1} + T_p$, we break down the
two integrals above in between these intervals:
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) &= \\
    V_i \big(\vect{e}_{1,i} (t_{k+1} + T_p)\big)
    &+ \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s \\
    -V_i \big(\vect{e}_{0,i} (t_k + T_p)\big)
    &- \int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
    - \int_{t_{k+1}}^{t_k + T_p} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
\label{eq:convergence_4_integrals}
\end{align}

\begin{gg_box}

Since no model mismatch or disturbances are present, consulting remark
\eqref{remark:predicted_actual_equations_without_disturbance} and substituting
for $\tau_0 = t_k$ and $\tau_1 = t_{k+1}$ yields:
\begin{align}
  \vect{e}_i\big(t_{k+1};\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big), \vect{e}_i(t_k)\big) &=
    \vect{e}_i(t_k) + \int_{t_k}^{t_{k+1}} g_i\big(\vect{e}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s)\big) ds \\
  \overline{\vect{e}}_i\big(t_{k+1};\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big), \vect{e}_i(t_k)\big) &=
    \vect{e}_i(t_k) + \int_{t_k}^{t_{k+1}} g_i\big(\overline{\vect{e}}_i(s;\ \vect{e}_i(t_k)), \overline{\vect{u}}_i^{\star}(s)\big) ds
\end{align}
hence
$\vect{e}_i\big(t_{k+1};\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big), \vect{e}_i(t_k)\big) =
\overline{\vect{e}}_i\big(t_{k+1};\ \overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big), \vect{e}_i(t_k)\big)$.

In between the times $t_{k+1}$ and $t_k + T_p$, the constructed feasible input
$\widetilde{\vect{u}}_i(\cdot)$ is equal to the optimal
input $\overline{\vect{u}}_i^{\star}\big(\cdot;\ \vect{e}_i(t_k)\big)$
(see eq. \ref{eq:optimal_input_t_plus_one}), which means that
$\vect{u}_{1,i}(\tau) = \vect{u}_{0,i}(\tau)$ in the interval $\tau \in [t_{k+1}, t_k + T_p]$.

the predicted states according to these inputs will
be also equal in this interval: $\vect{e}_{1,i}(\cdot) = \vect{e}_{0,i}(\cdot)$,
Hence, the following equality holds over $[t_{k+1}, t_k + T_p]$:
\begin{align}
  F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) =
  F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big),\ s \in [t_{k+1}, t_k + T_p]
\end{align}

Integrating this equality over the interval where it is valid yields

\begin{align}
  \int_{t_{k+1}}^{t_k + T_p}F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) ds =
  \int_{t_{k+1}}^{t_k + T_p}F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) ds
\end{align}

\end{gg_box}

This means that these two integrals featured in the right-hand side of eq.
\eqref{eq:convergence_4_integrals} vanish, and thus the cost difference becomes

\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) &= \\
    V_i \big(\vect{e}_{1,i} (t_{k+1} + T_p)\big)
    &+\int_{t_k + T_p}^{t_{k+1} + T_p} F_i \big(\vect{e}_{1,i}(s), \vect{u}_{1,i} (s)\big) d s \\
    -V_i \big(\vect{e}_{0,i} (t_k + T_p)\big)
    &-\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
\label{eq:convergence_2_integrals}
\end{align}

\begin{gg_box}
  We turn our attention to the first integral in the above expression, and we
  note that $(t_{k+1} + T_p) - (t_k + T_p) = t_{k+1} - t_k = h$, which is exactly the
  length of the interval where assumption (3b) of the theorem holds. Hence,
  we decide to integrate the expression found in the assumption over the
  interval $[t_{k+1} + T_p, t_k + T_p]$, for the controls and states that are
  applicable in it:
  \begin{align}
    \int_{t_k + T_p}^{t_{k+1} + T_p} \Bigg(\dfrac{\partial V_i}{\partial \vect{e}_{1,i}} g_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big)
    + F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big)\Bigg) ds \leq 0 \\
    \int_{t_k + T_p}^{t_{k+1} + T_p} \dfrac{d}{ds} V_i\big(\vect{e}_{1,i}(s)\big) d s
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds \leq 0 \\
    V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big) - V_i\big(\vect{e}_{1,i}(t_k + T_p)\big)
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds \leq 0 \\
    V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds \leq V_i\big(\vect{e}_{1,i}(t_k + T_p)\big)
  \end{align}

  The left-hand side expression is the same as the first two terms in the
  right-hand side of equality \eqref{eq:convergence_2_integrals}. We can
  introduce the third one by subtracting it from both sides of the above
  expression:
  \begin{align}
    V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
    - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \\
    \leq V_i\big(\vect{e}_{1,i}(t_k + T_p)\big)
    - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \\
    \leq \Big|V_i\big(\vect{e}_{1,i}(t_k + T_p)\big)
    - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big)\Big|
  \end{align}
  since $x \leq |x|, \forall x \in \mathbb{R}$.

  By revisiting lemma \eqref{lemma:V_Lipschitz_e_0}, the above inequality
  becomes
  \begin{align}
    V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
    - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \\
    \leq L_{V_i} \big\|\vect{e}_{1,i}(t_k + T_p) - \vect{e}_{0,i}(t_k + T_p)\big\|
  \end{align}

  But in the interval $[t_{k+1}, t_k + T_p]$:
  $\vect{e}_{1,i}(\cdot) = \vect{e}_{0,i}(\cdot)$ holds, hence the right-hand
  side of the inequality equals zero:
  \begin{align}
    V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds
    - V_i\big(\vect{e}_{0,i}(t_k + T_p)\big) \leq 0
  \end{align}

  By subtracting the term
  $\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s$
  from both sides we get
  \begin{align}
    &V_i\big(\vect{e}_{1,i}(t_{k+1} + T_p)\big)
    + \int_{t_k + T_p}^{t_{k+1} + T_p} F_i\big(\vect{e}_{1,i}(s), \vect{u}_{1,i}(s)\big) ds \\
    - &V_i\big(\vect{e}_{0,i}(t_k + T_p)\big)
    -\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
    \leq -\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) d s
  \end{align}

  The left-hand side of this inequality is now equal to the cost difference
  $\overline{J}_i\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big)$.
\end{gg_box}

Hence, the cost difference becomes
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) \leq
    -\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) ds
\end{align}

\begin{gg_box}
  $F_i$ is a positive-definite function as a sum of a positive-definite
  $\|\vect{u}_i\|^2_{\mat{R}_i}$ and a positive semi-definite function
  $\|\vect{e}_i\|^2_{\mat{Q}_i}$. If we denote by $m \geq 0$ the
  minimum eigenvalue between those of matrices $\mat{R}_i, \mat{Q}_i$, this
  means that
  $$F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) \geq m \|\vect{e}_{0,i}(s)\|^2$$

  By integrating the above between our interval of interest $[t_k, t_{k+1}]$ we get
  \begin{align}
    \int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) &\geq \int_{t_k}^{t_{k+1}} m \|\vect{e}_{0,i}(s)\|^2 ds \\
    -\int_{t_k}^{t_{k+1}} F_i \big(\vect{e}_{0,i}(s), \vect{u}_{0,i} (s)\big) &\leq -m \int_{t_k}^{t_{k+1}} \|\vect{e}_{0,i}(s)\|^2 ds
  \end{align}
\end{gg_box}

Which means that the cost difference becomes
\begin{align}
  \overline{J}_i\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big)
  &\leq -m \int_{t_k}^{t_{k+1}} \|\vect{e}_{0,i}(s)\|^2 ds \leq 0
\end{align}
and since the cost $\overline{J}_i\big(\vect{e}_i(t_{k+1})\big)$ is, in general,
sub-optimal: $J_i^{\star}\big(\vect{e}_i(t_{k+1})\big) \leq \overline{J}_i\big(\vect{e}_i(t_{k+1})\big)$,
which means that
\begin{align}
 J_i^{\star}\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) \leq -m \int_{t_k}^{t_{k+1}} \|\vect{e}_{0,i}(s)\|^2 ds
 \label{eq:J_opt_between_consecutive_k}
\end{align}
With this result established, we need to trace the time $t_k$ back to $t_0 = 0$.

\begin{gg_box}
The integral
  \begin{align}
    \int_{t_0}^{t_{k+1}} \|\vect{e}_{0,i}(s)\|^2 ds = \sum\limits_{j=0}^{k} \int_{t_j}^{t_{j+1}} \|\vect{e}_{0,i}(s)\|^2 ds
  \end{align}
  and the integral with limits from $t_0$ to $t_{k+1}$ can be decomposed into
  the addition of two integrals with limits from (a) $t_0$ to $t_k$ and (b)
  $t_k$ to $t_{k+1}$:
  \begin{align}
    \int_{t_0}^{t_{k+1}} \|\vect{e}_{0,i}(s)\|^2 ds = \int_{t_0}^{t_{k}} \|\vect{e}_{0,i}(s)\|^2 ds + \int_{t_k}^{t_{k+1}} \|\vect{e}_{0,i}(s)\|^2 ds
  \end{align}
  which means that
  \begin{align}
    \int_{t_k}^{t_{k+1}} \|\vect{e}_{0,i}(s)\|^2 ds = \int_{t_0}^{t_{k+1}} \|\vect{e}_{0,i}(s)\|^2 ds - \int_{t_0}^{t_{k}} \|\vect{e}_{0,i}(s)\|^2 ds
  \end{align}
  making the optimal cost difference between the consecutive sampling times
  $t_k$ and $t_{k+1}$ in \eqref{eq:J_opt_between_consecutive_k}
  \begin{align}
    J_i^{\star}\big(\vect{e}_i(t_{k+1})\big) - J_i^{\star}\big(\vect{e}_i(t_k)\big) \leq
      -m \int_{t_0}^{t_{k+1}} \|\vect{e}_{0,i}(s)\|^2 ds +m \int_{t_0}^{t_{k}} \|\vect{e}_{0,i}(s)\|^2 ds
  \end{align}
  Similarly, the optimal cost difference between the sampling times $t_{k-1}$
  and $t_{k}$ is
  \begin{align}
    J_i^{\star}\big(\vect{e}_i(t_{k})\big) - J_i^{\star}\big(\vect{e}_i(t_{k-1})\big) \leq
      -m \int_{t_0}^{t_{k}} \|\vect{e}_{0,i}(s)\|^2 ds +m \int_{t_0}^{t_{k-1}} \|\vect{e}_{0,i}(s)\|^2 ds
  \end{align}

  and we can apply this rationale all the way back to that between $t_0$ and
  $t_1$. Summing all the inequalities between the pairs of consecutive
  sampling times $(t_0, t_1)$, $(t_1, t_2)$, $\dots$, $(t_{k-1}, t_k)$, we get
  \begin{align}
    J_i^{\star}\big(\vect{e}_i(t_{k})\big) - J_i^{\star}\big(\vect{e}_i(t_0)\big) \leq
      -m \int_{t_0}^{t_{k}} \|\vect{e}_{0,i}(s)\|^2 ds
  \end{align}
\end{gg_box}

Hence, for $t_0 = 0$
\begin{align}
  J_i^{\star}\big(\vect{e}_i(t_{k})\big) - J_i^{\star}\big(\vect{e}_i(0)\big) \leq
    -m \int_{0}^{t_{k}} \|\vect{e}_{0,i}(s)\|^2 ds \leq 0
\label{eq:J_opt_between_k_and_0}
\end{align}
which implies that the value function $J_i^{\star}\big(\vect{e}_i(t_{k})\big)$
in non-increasing for all sampling times:
\begin{align}
  J_i^{\star}\big(\vect{e}_i(t_{k})\big) \leq J_i^{\star}\big(\vect{e}_i(0)\big),\ \forall t_k \in \mathbb{R}_{\geq0}
\end{align}
Let us now define the function $V_i(\vect{e}_i(t))$:
\begin{align}
  V_i\big(\vect{e}(t)\big) \triangleq J_i^{\star}\big(\vect{e}_i(\tau)\big) \leq J_i^{\star}\big(\vect{e}_i(0)\big),\ t \in \mathbb{R}_{\geq0}
\end{align}
where $\tau = max\{t_k : t_k \leq t\}$. Since $J_i^{\star}\big(\vect{e}_i(0)\big)$
is bounded, $V_i\big(\vect{e}(t)\big)$ is also bounded, which implies that the
signals $\vect{e}_i(t)$ and $\vect{u}_i(t)$ are also bounded. According to
\eqref{eq:position_based_error_model}, this means that $\dot{\vect{e}}_i(t)$
is bounded as well. From inequality \eqref{eq:J_opt_between_k_and_0} we then
have
\begin{align}
  V_i\big(\vect{e}_i(t)\big) = J_i^{\star}\big(\vect{e}_i(\tau)\big) \leq J_i^{\star}\big(\vect{e}_i(0)\big)
    -m \int_{0}^{\tau} \|\vect{e}_{0,i}(s)\|^2 ds \leq 0
\end{align}
which, due to the fact that $\tau \leq t$, is equivalent to
\begin{align}
  V_i\big(\vect{e}_i(t)\big) \leq J_i^{\star}\big(\vect{e}_i(0)\big) -m \int_{0}^{t} \|\vect{e}_{0,i}(s)\|^2 ds \leq 0, t \in \mathbb{R}_{t\geq 0}
\end{align}
Solving for the integral we get
\begin{align}
  \int_{0}^{t} \|\vect{e}_{0,i}(s)\|^2 ds \leq \dfrac{1}{m}\Big(J_i^{\star}\big(\vect{e}_i(0)\big) - V_i\big(\vect{e}_i(t)\big)\Big), t \in \mathbb{R}_{t\geq 0}
\end{align}
Both $J_i^{\star}\big(\vect{e}_i(0)\big)$ and $V_i\big(\vect{e}_i(t)\big)$
are bounded, and therefore so is their difference, which means that the
integral $\int\limits_{0}^{t} \|\vect{e}_{0,i}(s)\|^2 ds$ is bounded as well.
We make use of the following lemma to show that the error internal to the
norm of the integral goes to zero in steady-state:
\begin{bw_box}
  \begin{lemma} (\textit{A modification of Barbalat's lemma}\cite{Fontes2007})

    Let $f$ be a continuous, positive-definite function, and $\vect{x}$ be an
    absolutely continuous function in $\mathbb{R}$. If the following hold:
  \begin{itemize}
    \item $\|\vect{x}(\cdot)\| < \infty, \|\dot{\vect{x}}(\cdot)\| < \infty$
    \item $\lim\limits_{t \to \infty} \int\limits_0^t f\big(\vect{x}(s)\big) < \infty$
  \end{itemize}
  then $\lim\limits_{t \to \infty} \|\vect{x}(t)\| = 0$
  \label{lemma:barbalat}
  \end{lemma}
\end{bw_box}
Lemma \eqref{lemma:barbalat} assures us that under these conditions for the
error and its dynamics, which are fulfilled in our case, the error
\begin{align}
  \lim\limits_{t \to \infty}&\|\vect{e}_{0,i}(t)\| \to 0 \Leftrightarrow \\
  \lim\limits_{t \to \infty}
  &\Big\| \overline{\vect{e}}_i\Big(t;\ \overline{\vect{u}}_i^{\star}\big(t;\ \vect{e}_i(t_k)\big), \vect{e}_i(t_k) \Big) \Big\| \to 0,\
\forall t_k \in \mathbb{R}_{\geq 0}
\end{align}
which, given \eqref{eq:error_now_to_predicted_error}, means that
$$\lim\limits_{t \to \infty}\|\vect{e}_i(t)\| \to 0$$

Therefore, the closed-loop trajectory of $\vect{e}_i$ converges to the terminal
set $\mathcal{E}_{i,f}$ as $t \to \infty$, and the proof is concluded. \qedsymbol
